{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46c582c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "549609f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gensim\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import OrderedDict\n",
    "from torch import nn, optim\n",
    "from torchtext.data import utils\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbf3b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import AbstractDataset\n",
    "from util import load_model_and_opt, save_model, batch_predict\n",
    "from cnn_util import CNNBase3, train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ef00680",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = (f'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "HOME = '/home/hice1/khom9/CSE 8803 BMI Final Project'\n",
    "EMBED_KEYS_PATH = f'{HOME}/wordvectors/abstracts200_normalized.wordvectors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e39c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type of CNN (1, 2, or 3); refer to cnn_util.py for more info\n",
    "VERSION = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4deebd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n",
      "Tesla V100-PCIE-32GB\n"
     ]
    }
   ],
   "source": [
    "print(f'Using device {DEVICE}')\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9b06e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/hice1/khom9/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/usr/local/lib/python3.10/dist-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Only run once\n",
    "'''\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "df = pd.read_csv('CleanedAVdata.csv')\n",
    "nltk.download('wordnet')\n",
    "tk = utils.get_tokenizer('spacy')\n",
    "null_word = '\\0'\n",
    "lemma = WordNetLemmatizer()\n",
    "abstracts = df['Abstract']\n",
    "embed_dim = 200\n",
    "tokens = pd.Series([[lemma.lemmatize(w) for w in tk(abst)] for abst in abstracts])\n",
    "model = Word2Vec(sentences=tokens, vector_size=embed_dim, window=5, min_count=1, workers=12)\n",
    "\n",
    "mu = np.mean(model.wv.vectors)\n",
    "sigma = np.sqrt(np.var(model.wv.vectors))\n",
    "model.wv.vectors = (1 + (np.clip(model.wv.vectors, mu-3*sigma, mu+3*sigma) - mu) / (3*sigma)) / 2\n",
    "model.wv.vectors = np.clip(model.wv.vectors, 0, 1)\n",
    "\n",
    "model.wv[null_word] = np.zeros(embed_dim)\n",
    "model.wv.save(f'{HOME}/wordvectors/abstracts{embed_dim}_normalized.wordvectors')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8654b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(\n",
      "100%|██████████| 23250/23250 [00:20<00:00, 1139.42it/s]\n"
     ]
    }
   ],
   "source": [
    "tk = utils.get_tokenizer('spacy')\n",
    "wv = gensim.models.KeyedVectors.load(EMBED_KEYS_PATH, mmap='r')\n",
    "null_word = '\\0'\n",
    "d = AbstractDataset(f'{HOME}/CleanedAVdata.csv', 'Abstract', 'IPCR Classifications', tk, wv.key_to_index,\n",
    "                    null_word=null_word, min_len=30, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caa4cf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CNNBase2(EMBED_KEYS_PATH, null_word=null_word).to(DEVICE)\n",
    "# loader = DataLoader(d, batch_size=31, shuffle=True)\n",
    "# txt, label = next(iter(loader))\n",
    "# txt = txt.to(DEVICE)\n",
    "# model(txt.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fa7bf00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 48\n",
    "lr = 1e-4\n",
    "\n",
    "save_path = f'{HOME}/models/cnn_model-{VERSION}.pth'\n",
    "act_path = f'{HOME}/models/cnn_model-{VERSION}-max-activations.pkl'\n",
    "wv_out_path = f'{HOME}/wordvectors/abstracts200_trained_normalized_{VERSION}.wordvectors'\n",
    "\n",
    "model = CNNBase3(EMBED_KEYS_PATH, null_word=null_word).to(DEVICE)\n",
    "\n",
    "num_pos = d.labels.sum(axis=0, keepdim=True).to_dense()\n",
    "pos_weight = (d.labels.shape[0] - num_pos) / num_pos\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(DEVICE))\n",
    "optimizer = optim.NAdam(model.parameters(), lr=lr)\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "\n",
    "# model, optimizer = load_model_and_opt(model, optimizer, save_path)\n",
    "# for param_group in optimizer.param_groups:\n",
    "#         param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffe5e3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0001\n",
      "Training for 25 epochs, with batch size=48\n",
      "Using device: cuda:0\n",
      "Saving model every 25 epochs to /home/hice1/khom9/CSE 8803 BMI Final Project/models/cnn_model-999.pth\n",
      "\n",
      "-----Epoch 1/25-----\n",
      "Batch 150/485, loss: 0.28314600254098576 (0.987s)\n",
      "Batch 300/485, loss: 0.3028059339523315 (0.988s)\n",
      "Batch 450/485, loss: 0.2838246899843216 (0.991s)\n",
      "Batch 485/485, loss: 0.26886808446475435 (0.230s)\n",
      "F1 score: 0.5004786081969066\n",
      "\n",
      "-----Epoch 2/25-----\n",
      "Batch 150/485, loss: 0.2522989002863566 (0.989s)\n",
      "Batch 300/485, loss: 0.28299306213855746 (0.986s)\n",
      "Batch 450/485, loss: 0.30251906712849935 (0.987s)\n",
      "Batch 485/485, loss: 0.2942827011857714 (0.230s)\n",
      "F1 score: 0.5041616204133762\n",
      "\n",
      "-----Epoch 3/25-----\n",
      "Batch 150/485, loss: 0.262884119451046 (0.992s)\n",
      "Batch 300/485, loss: 0.2576737888654073 (0.991s)\n",
      "Batch 450/485, loss: 0.24624839713176092 (0.995s)\n",
      "Batch 485/485, loss: 0.24074789094073432 (0.232s)\n",
      "F1 score: 0.5086417264595164\n",
      "\n",
      "-----Epoch 4/25-----\n",
      "Batch 150/485, loss: 0.2388402067621549 (0.994s)\n",
      "Batch 300/485, loss: 0.2785159567991892 (0.993s)\n",
      "Batch 450/485, loss: 0.2514329112569491 (0.994s)\n",
      "Batch 485/485, loss: 0.24971113886151994 (0.232s)\n",
      "F1 score: 0.5102418585854652\n",
      "\n",
      "-----Epoch 5/25-----\n",
      "Batch 150/485, loss: 0.2226975558201472 (0.993s)\n",
      "Batch 300/485, loss: 0.24061471025149028 (0.992s)\n",
      "Batch 450/485, loss: 0.24360039646426837 (0.994s)\n",
      "Batch 485/485, loss: 0.2242060797555106 (0.231s)\n",
      "F1 score: 0.514458751462428\n",
      "\n",
      "-----Epoch 6/25-----\n",
      "Batch 150/485, loss: 0.22211723471681277 (0.994s)\n",
      "Batch 300/485, loss: 0.23791618436574935 (0.991s)\n",
      "Batch 450/485, loss: 0.229469784895579 (0.993s)\n",
      "Batch 485/485, loss: 0.22361145189830234 (0.232s)\n",
      "F1 score: 0.5164279824340533\n",
      "\n",
      "-----Epoch 7/25-----\n",
      "Batch 150/485, loss: 0.21566157167156538 (0.994s)\n",
      "Batch 300/485, loss: 0.21064751048882802 (0.992s)\n",
      "Batch 450/485, loss: 0.21309538334608077 (0.994s)\n",
      "Batch 485/485, loss: 0.22164233326911925 (0.232s)\n",
      "F1 score: 0.5207328770464869\n",
      "\n",
      "-----Epoch 8/25-----\n",
      "Batch 150/485, loss: 0.2191569186747074 (0.993s)\n",
      "Batch 300/485, loss: 0.19707657963037492 (0.993s)\n",
      "Batch 450/485, loss: 0.20299420287211736 (0.994s)\n",
      "Batch 485/485, loss: 0.22459497643368584 (0.232s)\n",
      "F1 score: 0.5225717380933347\n",
      "\n",
      "-----Epoch 9/25-----\n",
      "Batch 150/485, loss: 0.19335357238849005 (0.992s)\n",
      "Batch 300/485, loss: 0.21012946685155232 (0.991s)\n",
      "Batch 450/485, loss: 0.20590985347827276 (0.993s)\n",
      "Batch 485/485, loss: 0.1999384765114103 (0.232s)\n",
      "F1 score: 0.5257354257665264\n",
      "\n",
      "-----Epoch 10/25-----\n",
      "Batch 150/485, loss: 0.20145547946294148 (0.993s)\n",
      "Batch 300/485, loss: 0.19883933121959368 (0.993s)\n",
      "Batch 450/485, loss: 0.21683266545335453 (0.994s)\n",
      "Batch 485/485, loss: 0.19135251215526036 (0.234s)\n",
      "F1 score: 0.5236873320748013\n",
      "\n",
      "-----Epoch 11/25-----\n",
      "Batch 150/485, loss: 0.1963610379397869 (0.995s)\n",
      "Batch 300/485, loss: 0.18149108812212944 (0.993s)\n",
      "Batch 450/485, loss: 0.18675092870990434 (0.994s)\n",
      "Batch 485/485, loss: 0.20698498210736682 (0.232s)\n",
      "F1 score: 0.531014965295114\n",
      "\n",
      "-----Epoch 12/25-----\n",
      "Batch 150/485, loss: 0.2016217920680841 (0.994s)\n",
      "Batch 300/485, loss: 0.18041111374894778 (0.992s)\n",
      "Batch 450/485, loss: 0.18028623163700103 (0.992s)\n",
      "Batch 485/485, loss: 0.19441914537123273 (0.231s)\n",
      "F1 score: 0.532492965263609\n",
      "\n",
      "-----Epoch 13/25-----\n",
      "Batch 150/485, loss: 0.16986551031470298 (0.992s)\n",
      "Batch 300/485, loss: 0.17257567942142488 (0.991s)\n",
      "Batch 450/485, loss: 0.1818536506096522 (0.991s)\n",
      "Batch 485/485, loss: 0.18720777864967073 (0.232s)\n",
      "F1 score: 0.5363971224136945\n",
      "\n",
      "-----Epoch 14/25-----\n",
      "Batch 150/485, loss: 0.18046853298942248 (0.994s)\n",
      "Batch 300/485, loss: 0.16758187706271807 (0.993s)\n",
      "Batch 450/485, loss: 0.17240349128842353 (0.993s)\n",
      "Batch 485/485, loss: 0.1805720301611083 (0.231s)\n",
      "F1 score: 0.5376171419003525\n",
      "\n",
      "-----Epoch 15/25-----\n",
      "Batch 150/485, loss: 0.1672019475698471 (0.995s)\n",
      "Batch 300/485, loss: 0.1753814123570919 (0.990s)\n",
      "Batch 450/485, loss: 0.16799389104048412 (0.994s)\n",
      "Batch 485/485, loss: 0.17253327412264688 (0.231s)\n",
      "F1 score: 0.5393015014737838\n",
      "\n",
      "-----Epoch 16/25-----\n",
      "Batch 150/485, loss: 0.16356267601251603 (0.992s)\n",
      "Batch 300/485, loss: 0.16289049923419952 (0.990s)\n",
      "Batch 450/485, loss: 0.16505586296319963 (0.992s)\n",
      "Batch 485/485, loss: 0.1644800978047507 (0.231s)\n",
      "F1 score: 0.5417428681607586\n",
      "\n",
      "-----Epoch 17/25-----\n",
      "Batch 150/485, loss: 0.15927219395836195 (0.992s)\n",
      "Batch 300/485, loss: 0.45177553792794545 (0.990s)\n",
      "Batch 450/485, loss: 0.35330774625142414 (0.992s)\n",
      "Batch 485/485, loss: 0.37810374540942054 (0.231s)\n",
      "F1 score: 0.4828181144626898\n",
      "\n",
      "-----Epoch 18/25-----\n",
      "Batch 150/485, loss: 0.3490092085798581 (0.992s)\n",
      "Batch 300/485, loss: 0.2634671649336815 (0.990s)\n",
      "Batch 450/485, loss: 0.24162524402141572 (0.993s)\n",
      "Batch 485/485, loss: 0.24244917460850307 (0.231s)\n",
      "F1 score: 0.4891139363630881\n",
      "\n",
      "-----Epoch 19/25-----\n",
      "Batch 150/485, loss: 0.3243557154138883 (0.992s)\n",
      "Batch 300/485, loss: 0.26223421533902486 (0.991s)\n",
      "Batch 450/485, loss: 0.20446107909083366 (0.993s)\n",
      "Batch 485/485, loss: 0.19059120884963443 (0.232s)\n",
      "F1 score: 0.5025131779236012\n",
      "\n",
      "-----Epoch 20/25-----\n",
      "Batch 150/485, loss: 0.18895780583222707 (0.992s)\n",
      "Batch 300/485, loss: 0.18311799690127373 (0.991s)\n",
      "Batch 450/485, loss: 0.1701270472506682 (0.991s)\n",
      "Batch 485/485, loss: 0.18062462891851153 (0.232s)\n",
      "F1 score: 0.531248416016602\n",
      "\n",
      "-----Epoch 21/25-----\n",
      "Batch 150/485, loss: 0.16062597488363584 (0.992s)\n",
      "Batch 300/485, loss: 0.1553134612242381 (0.992s)\n",
      "Batch 450/485, loss: 0.15501141021649043 (0.993s)\n",
      "Batch 485/485, loss: 0.15998267765556062 (0.231s)\n",
      "F1 score: 0.5442621223026736\n",
      "\n",
      "-----Epoch 22/25-----\n",
      "Batch 150/485, loss: 0.1491920407116413 (0.992s)\n",
      "Batch 300/485, loss: 0.1413405604660511 (0.991s)\n",
      "Batch 450/485, loss: 0.15049968004226685 (0.992s)\n",
      "Batch 485/485, loss: 0.14848823909248623 (0.231s)\n",
      "F1 score: 0.5491213060049268\n",
      "\n",
      "-----Epoch 23/25-----\n",
      "Batch 150/485, loss: 0.13385945320129394 (0.992s)\n",
      "Batch 300/485, loss: 0.1435132847726345 (0.990s)\n",
      "Batch 450/485, loss: 0.1453792479634285 (0.993s)\n",
      "Batch 485/485, loss: 0.15781224348715372 (0.231s)\n",
      "F1 score: 0.5538460951195061\n",
      "\n",
      "-----Epoch 24/25-----\n",
      "Batch 150/485, loss: 0.1436344237625599 (0.993s)\n",
      "Batch 300/485, loss: 0.13588323002060254 (0.990s)\n",
      "Batch 450/485, loss: 0.13300846735636393 (0.993s)\n",
      "Batch 485/485, loss: 0.13388703359024864 (0.231s)\n",
      "F1 score: 0.5564920669670798\n",
      "\n",
      "-----Epoch 25/25-----\n",
      "Batch 150/485, loss: 0.1314659776786963 (0.993s)\n",
      "Batch 300/485, loss: 0.13081925173600514 (0.990s)\n",
      "Batch 450/485, loss: 0.1357175769408544 (0.991s)\n",
      "Batch 485/485, loss: 0.1250097136412348 (0.232s)\n",
      "Saved to /home/hice1/khom9/CSE 8803 BMI Final Project/models/cnn_model-999.pth\n",
      "F1 score: 0.5599078016763008\n"
     ]
    }
   ],
   "source": [
    "epochs = 350\n",
    "train_model(model, optimizer, d, loss_fn, epochs=epochs, batch_size=batch_size, save_freq=25, \n",
    "            save_path=save_path, scheduler=scheduler, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e6ef550",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save_model(save_path, model, optimizer, epochs)\n",
    "# print(f'Saved to {save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1a0c335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39733967185020447\n",
      "tensor([[ 2.2486e-01,  0.0000e+00],\n",
      "        [-7.9525e+01,  0.0000e+00],\n",
      "        [-1.6868e+01,  0.0000e+00],\n",
      "        [-6.4548e+01,  0.0000e+00],\n",
      "        [-7.1431e+01,  0.0000e+00],\n",
      "        [-4.2384e+01,  0.0000e+00],\n",
      "        [-6.1352e+01,  0.0000e+00],\n",
      "        [ 1.2967e+00,  0.0000e+00],\n",
      "        [-1.6110e-01,  0.0000e+00],\n",
      "        [-1.6940e+01,  0.0000e+00],\n",
      "        [-1.0913e+00,  0.0000e+00],\n",
      "        [-2.4919e+01,  0.0000e+00],\n",
      "        [-8.1480e+01,  0.0000e+00],\n",
      "        [-1.3655e+01,  0.0000e+00],\n",
      "        [-5.9665e+01,  0.0000e+00],\n",
      "        [-8.3912e+01,  0.0000e+00],\n",
      "        [-9.3107e+00,  0.0000e+00],\n",
      "        [-5.7356e+01,  0.0000e+00],\n",
      "        [-8.8922e+01,  0.0000e+00],\n",
      "        [-2.9137e+01,  0.0000e+00],\n",
      "        [-1.9960e-01,  0.0000e+00],\n",
      "        [-9.2277e+01,  0.0000e+00],\n",
      "        [-8.0748e+01,  0.0000e+00],\n",
      "        [-7.3573e+01,  0.0000e+00],\n",
      "        [-5.6682e+01,  0.0000e+00],\n",
      "        [-9.8003e+01,  0.0000e+00],\n",
      "        [-4.9442e+01,  0.0000e+00],\n",
      "        [-5.5357e+01,  0.0000e+00],\n",
      "        [-6.7284e+01,  0.0000e+00],\n",
      "        [-6.5950e+01,  0.0000e+00],\n",
      "        [ 4.5869e-01,  0.0000e+00],\n",
      "        [-1.4266e-01,  0.0000e+00],\n",
      "        [ 1.0338e+00,  0.0000e+00],\n",
      "        [ 1.2575e+00,  0.0000e+00],\n",
      "        [ 6.8524e-01,  0.0000e+00],\n",
      "        [ 2.9362e-01,  0.0000e+00],\n",
      "        [ 1.5670e+00,  0.0000e+00],\n",
      "        [-2.4172e+01,  0.0000e+00],\n",
      "        [-8.1128e+01,  0.0000e+00],\n",
      "        [-9.6022e+01,  0.0000e+00],\n",
      "        [-5.0816e+01,  0.0000e+00],\n",
      "        [-2.7251e+01,  0.0000e+00],\n",
      "        [-4.0562e+01,  0.0000e+00],\n",
      "        [-7.6798e+01,  0.0000e+00],\n",
      "        [-7.0067e+01,  0.0000e+00],\n",
      "        [-5.2015e+01,  0.0000e+00],\n",
      "        [-8.9698e+01,  0.0000e+00],\n",
      "        [-6.9462e+01,  0.0000e+00],\n",
      "        [-8.9089e+01,  0.0000e+00],\n",
      "        [-8.9691e+01,  0.0000e+00],\n",
      "        [-7.5960e+01,  0.0000e+00],\n",
      "        [-9.3245e+01,  0.0000e+00],\n",
      "        [-1.8433e+01,  0.0000e+00],\n",
      "        [-1.4019e+00,  0.0000e+00],\n",
      "        [-1.3465e+00,  0.0000e+00],\n",
      "        [-6.0736e+01,  0.0000e+00],\n",
      "        [ 5.7061e+00,  0.0000e+00],\n",
      "        [ 1.7465e-02,  0.0000e+00],\n",
      "        [-2.1067e+01,  0.0000e+00],\n",
      "        [-2.9684e+01,  0.0000e+00],\n",
      "        [-3.2806e+00,  0.0000e+00],\n",
      "        [ 2.4229e+00,  0.0000e+00],\n",
      "        [-7.8711e+01,  0.0000e+00],\n",
      "        [-5.9897e+01,  0.0000e+00],\n",
      "        [-5.1138e+01,  0.0000e+00],\n",
      "        [ 1.2873e+00,  0.0000e+00],\n",
      "        [-3.7378e+01,  0.0000e+00],\n",
      "        [-3.3439e+00,  0.0000e+00],\n",
      "        [-2.6987e+01,  0.0000e+00],\n",
      "        [-1.6048e+01,  0.0000e+00],\n",
      "        [-1.0214e+02,  0.0000e+00],\n",
      "        [-5.3685e+01,  0.0000e+00],\n",
      "        [-4.3582e+01,  0.0000e+00],\n",
      "        [-7.5121e+01,  0.0000e+00],\n",
      "        [-9.8199e-01,  0.0000e+00],\n",
      "        [-1.6383e+00,  0.0000e+00],\n",
      "        [ 4.1529e+00,  1.0000e+00],\n",
      "        [-4.5665e+00,  0.0000e+00],\n",
      "        [-1.1897e+00,  0.0000e+00],\n",
      "        [-2.0950e+00,  0.0000e+00],\n",
      "        [-1.7345e+00,  0.0000e+00],\n",
      "        [-1.8935e+00,  0.0000e+00],\n",
      "        [-3.1258e+00,  0.0000e+00],\n",
      "        [-4.5253e+00,  0.0000e+00],\n",
      "        [-7.2143e+00,  0.0000e+00],\n",
      "        [-3.6810e+00,  0.0000e+00],\n",
      "        [-8.0893e-01,  0.0000e+00],\n",
      "        [ 4.7657e-01,  0.0000e+00],\n",
      "        [-2.2579e+00,  0.0000e+00],\n",
      "        [-2.2679e+00,  0.0000e+00],\n",
      "        [ 2.9814e+00,  0.0000e+00]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "i = 8721 #18622\n",
    "txt, label = d[i]\n",
    "label = label.unsqueeze(0)\n",
    "print(loss_fn(model(txt.to(DEVICE)).detach(), label.to(DEVICE)).item())\n",
    "print(torch.cat([model(txt.to(DEVICE)).detach(), label.to(DEVICE)]).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e36ea3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = (torch.sigmoid(batch_predict(model, d.abst_data, device=DEVICE).detach().cpu()) > 0.5).type(torch.float)\n",
    "true = d.labels.to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "831c7a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total avg loss: 1.0110183169431584\n"
     ]
    }
   ],
   "source": [
    "total_loss = []\n",
    "loss_fn_cpu = loss_fn.cpu()\n",
    "for i in range(len(d)):\n",
    "    total_loss.append(loss_fn_cpu(pred[i].unsqueeze(0), true[i].unsqueeze(0)).item())\n",
    "    \n",
    "print(f'Total avg loss: {np.mean(total_loss)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f669161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11919    98.782166\n",
       "130      89.474243\n",
       "2267     88.225616\n",
       "22573    85.160156\n",
       "18414    85.030014\n",
       "17995    84.579231\n",
       "18534    82.953407\n",
       "18548    81.760132\n",
       "21263    81.263100\n",
       "18622    81.229805\n",
       "17761    81.229156\n",
       "18642    81.198997\n",
       "20998    44.929916\n",
       "11239    44.679024\n",
       "18483    41.625961\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.Series(total_loss)\n",
    "x.sort_values(ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d865e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14389234 0.06666667 0.11538462 0.09090909 0.09375    0.04166667\n",
      " 0.06666667 0.22026144 0.11806256 0.07428571 0.11642157 0.09813084\n",
      " 0.06976744 0.10714286 0.10638298 0.05       0.08112324 0.11538462\n",
      " 0.06666667 0.12539185 0.16653061 0.1        0.06451613 0.0625\n",
      " 0.1182266  0.05263158 0.0990991  0.0942029  0.07142857 0.06666667\n",
      " 0.75678195 0.08536585 0.22612011 0.07333333 0.11928105 0.11532385\n",
      " 0.09731877 0.11827957 0.06451613 0.08474576 0.04651163 0.28571429\n",
      " 0.05747126 0.0625     0.0625     0.09574468 0.08333333 0.07142857\n",
      " 0.07142857 0.07142857 0.04545455 0.08333333 0.06989247 0.06929134\n",
      " 0.07485605 0.05882353 0.06843575 0.09134045 0.08333333 0.06825939\n",
      " 0.12142857 0.1147086  0.09302326 0.10240964 0.05645161 0.13632843\n",
      " 0.09375    0.16717325 0.06962025 0.06859206 0.0952381  0.07692308\n",
      " 0.06179775 0.0375     0.3327763  0.13735071 0.12098765 0.09259259\n",
      " 0.52519073 0.59329473 0.17888199 0.28227571 0.12844037 0.13660377\n",
      " 0.1375     0.09490085 0.14265233 0.16076611 0.191067   0.35893032\n",
      " 0.11336516]\n",
      "Total precision: 0.4886579850191909\n"
     ]
    }
   ],
   "source": [
    "print(precision_score(true, pred, average=None))\n",
    "print(f'Total precision: {precision_score(true, pred, average=\"weighted\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f164fc17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97887324 1.         1.         1.         1.         1.\n",
      " 1.         0.99410029 0.97771588 1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         0.99029126 1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.66969147 1.         0.82879106 1.         0.93890675 1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.99435028\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         0.8185725  0.99606299 1.         1.\n",
      " 0.76232762 0.74304094 0.85714286 0.76677045 0.98054475 1.\n",
      " 1.         1.         1.         0.99640288 1.         0.84008039\n",
      " 1.        ]\n",
      "Total recall: 0.7743511998563763\n"
     ]
    }
   ],
   "source": [
    "print(recall_score(true, pred, average=None))\n",
    "print(f'Total recall: {recall_score(true, pred, average=\"weighted\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5a9f732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5577383648059518\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(true, pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "851d848b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total accuracy: 0.0636989247311828\n"
     ]
    }
   ],
   "source": [
    "print(f'Total accuracy: {accuracy_score(true, pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fca1506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved word embeddings to /home/hice1/khom9/CSE 8803 BMI Final Project/wordvectors/abstracts200_trained_normalized_999.wordvectors\n"
     ]
    }
   ],
   "source": [
    "wv_tuned = gensim.models.KeyedVectors.load(EMBED_KEYS_PATH, mmap='r')\n",
    "wv_tuned.vectors = model.embedding.weight.data.detach().cpu().numpy()\n",
    "\n",
    "wv_tuned.vectors = np.clip(wv_tuned.vectors, a_min=0, a_max=1.)\n",
    "\n",
    "wv_tuned.save(wv_out_path)\n",
    "print(f'Saved word embeddings to {wv_out_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4de0445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote max layer activations to /home/hice1/khom9/CSE 8803 BMI Final Project/models/cnn_model-999-max-activations.pkl\n"
     ]
    }
   ],
   "source": [
    "activations = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for txt, label in (d):\n",
    "        outputs = list(model(txt.to(DEVICE), all_outputs_max=True))\n",
    "        activations.append(outputs)\n",
    "activations = torch.tensor(activations)\n",
    "\n",
    "max_act = torch.max(activations, axis=0)[0]\n",
    "max_act_dict = OrderedDict(zip(list(dict(model.named_modules()).keys())[1:], max_act))\n",
    "\n",
    "output = open(act_path, 'wb')\n",
    "pickle.dump(max_act_dict, output)\n",
    "output.close()\n",
    "print(f'Wrote max layer activations to {act_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8723c4ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
