{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4db4fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "549609f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import OrderedDict\n",
    "from torch import nn, optim\n",
    "from torchtext.data import utils\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbf3b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import AbstractDataset\n",
    "from util import save_model, load_model_and_opt, batch_predict\n",
    "from cnn_util import CNNBase1, train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ef00680",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = (f'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "HOME = '/home/hice1/khom9/CSE 8803 BMI Final Project'\n",
    "EMBED_KEYS_PATH = f'{HOME}/wordvectors/abstracts200_normalized.wordvectors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9800acbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type of CNN (1, 2, or 3); refer to cnn_util.py for more info\n",
    "VERSION = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d11360d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n",
      "NVIDIA L40S\n"
     ]
    }
   ],
   "source": [
    "print(f'Using device {DEVICE}')\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9b06e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf = pd.read_csv('CleanedAVdata.csv')\\nnltk.download('wordnet')\\ntk = utils.get_tokenizer('spacy')\\nlemma = WordNetLemmatizer()\\nabstracts = df['Abstract']\\nembed_dim = 200\\ntokens = pd.Series([[lemma.lemmatize(w) for w in tk(abst)] for abst in abstracts])\\nmodel = Word2Vec(sentences=tokens, vector_size=embed_dim, window=5, min_count=1, workers=12)\\n\\nmu = np.mean(model.wv.vectors)\\nsigma = np.sqrt(np.var(model.wv.vectors))\\nmodel.wv.vectors = (1 + (np.clip(model.wv.vectors, mu-3*sigma, mu+3*sigma) - mu) / (3*sigma)) / 2\\nmodel.wv.vectors = np.clip(model.wv.vectors, 0, 1)\\n\\nmodel.wv['\\x00'] = np.zeros(embed_dim)\\nmodel.wv.save(EMBED_KEYS_PATH)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only run once\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "'''\n",
    "df = pd.read_csv('CleanedAVdata.csv')\n",
    "nltk.download('wordnet')\n",
    "tk = utils.get_tokenizer('spacy')\n",
    "lemma = WordNetLemmatizer()\n",
    "abstracts = df['Abstract']\n",
    "embed_dim = 200\n",
    "tokens = pd.Series([[lemma.lemmatize(w) for w in tk(abst)] for abst in abstracts])\n",
    "model = Word2Vec(sentences=tokens, vector_size=embed_dim, window=5, min_count=1, workers=12)\n",
    "\n",
    "mu = np.mean(model.wv.vectors)\n",
    "sigma = np.sqrt(np.var(model.wv.vectors))\n",
    "model.wv.vectors = (1 + (np.clip(model.wv.vectors, mu-3*sigma, mu+3*sigma) - mu) / (3*sigma)) / 2\n",
    "model.wv.vectors = np.clip(model.wv.vectors, 0, 1)\n",
    "\n",
    "model.wv['\\0'] = np.zeros(embed_dim)\n",
    "model.wv.save(EMBED_KEYS_PATH)\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8654b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(\n",
      "100%|██████████| 23250/23250 [00:15<00:00, 1497.28it/s]\n"
     ]
    }
   ],
   "source": [
    "tk = utils.get_tokenizer('spacy')\n",
    "wv = gensim.models.KeyedVectors.load(EMBED_KEYS_PATH, mmap='r')\n",
    "null_word = '\\0'\n",
    "d = AbstractDataset('CleanedAVdata.csv', 'Abstract', 'IPCR Classifications', tk, wv.key_to_index,\n",
    "                    null_word=null_word, min_len=30, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fa7bf00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "batch_size = 48\n",
    "model = CNNBase1(EMBED_KEYS_PATH, null_word=null_word).to(DEVICE)\n",
    "\n",
    "save_path = f'{HOME}/models/cnn_model-{VERSION}.pth'\n",
    "act_path = f'{HOME}/models/cnn_model-{VERSION}-max-activations.pkl'\n",
    "wv_out_path = f'{HOME}/wordvectors/abstracts200_trained_normalized_{VERSION}.wordvectors'\n",
    "\n",
    "num_pos = d.labels.sum(axis=0, keepdim=True).to_dense()\n",
    "pos_weight = (d.labels.shape[0] - num_pos) / num_pos\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(DEVICE))\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# model, optimizer = load_model_and_opt(model, optimizer, save_path)\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe5e3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.001\n",
      "Scheduler: <torch.optim.lr_scheduler.StepLR object at 0x15542eaef580>\n",
      "Training for 250 epochs, with batch size=48\n",
      "Using device: cuda:0\n",
      "Saving model every 25 epochs to /home/hice1/khom9/CSE 8803 BMI Final Project/models/cnn_model-1.pth\n",
      "\n",
      "-----Epoch 1/250-----\n",
      "Batch 150/485, loss: 1.407383065422376 (0.896s)\n",
      "Batch 300/485, loss: 1.3677325503031412 (0.497s)\n",
      "Batch 450/485, loss: 1.5280050901571909 (0.498s)\n",
      "Batch 485/485, loss: 1.2095337101391384 (0.124s)\n",
      "F1 score: 0.3848852972334508\n",
      "\n",
      "-----Epoch 2/250-----\n",
      "Batch 150/485, loss: 1.4273061811923982 (0.497s)\n",
      "Batch 300/485, loss: 1.2728089169661203 (0.494s)\n",
      "Batch 450/485, loss: 1.3472783998648326 (0.494s)\n",
      "Batch 485/485, loss: 1.3144826429230827 (0.116s)\n",
      "F1 score: 0.4018547505709647\n",
      "\n",
      "-----Epoch 3/250-----\n",
      "Batch 150/485, loss: 1.1586296979586284 (0.495s)\n",
      "Batch 300/485, loss: 1.0808591693639755 (0.492s)\n",
      "Batch 450/485, loss: 1.3127974275747936 (0.492s)\n",
      "Batch 485/485, loss: 1.0964189972196305 (0.115s)\n",
      "F1 score: 0.4317589272078272\n",
      "\n",
      "-----Epoch 4/250-----\n",
      "Batch 150/485, loss: 1.1200774125258128 (0.494s)\n",
      "Batch 300/485, loss: 1.0848438105980556 (0.491s)\n",
      "Batch 450/485, loss: 0.9091748962799708 (0.492s)\n",
      "Batch 485/485, loss: 1.0273271688393184 (0.115s)\n",
      "F1 score: 0.43874186252837405\n",
      "\n",
      "-----Epoch 5/250-----\n",
      "Batch 150/485, loss: 0.9306613536675771 (0.493s)\n",
      "Batch 300/485, loss: 0.9903192265828451 (0.492s)\n",
      "Batch 450/485, loss: 0.9079951250553131 (0.491s)\n",
      "Batch 485/485, loss: 1.1842296540737152 (0.115s)\n",
      "F1 score: 0.4326302065357958\n",
      "\n",
      "-----Epoch 6/250-----\n",
      "Batch 150/485, loss: 0.9603536271055539 (0.492s)\n",
      "Batch 300/485, loss: 0.8424507025877634 (0.491s)\n",
      "Batch 450/485, loss: 0.8705880673726399 (0.490s)\n",
      "Batch 485/485, loss: 0.7638850365366254 (0.115s)\n",
      "F1 score: 0.4319727544855964\n",
      "\n",
      "-----Epoch 7/250-----\n",
      "Batch 150/485, loss: 0.7655828142166138 (0.492s)\n",
      "Batch 300/485, loss: 0.8101103898882865 (0.490s)\n",
      "Batch 450/485, loss: 0.8443632664283117 (0.490s)\n",
      "Batch 485/485, loss: 0.6486113739865167 (0.115s)\n",
      "F1 score: 0.4301276630466317\n",
      "\n",
      "-----Epoch 8/250-----\n",
      "Batch 150/485, loss: 0.7437668852011363 (0.492s)\n",
      "Batch 300/485, loss: 0.6544478883345922 (0.490s)\n",
      "Batch 450/485, loss: 0.8074936825037002 (0.490s)\n",
      "Batch 485/485, loss: 0.6379450125353677 (0.115s)\n",
      "F1 score: 0.43550647920144225\n",
      "\n",
      "-----Epoch 9/250-----\n",
      "Batch 150/485, loss: 0.6808750801285108 (0.492s)\n",
      "Batch 300/485, loss: 0.6383366122841835 (0.491s)\n",
      "Batch 450/485, loss: 0.6759947408239046 (0.490s)\n",
      "Batch 485/485, loss: 0.742475808944021 (0.115s)\n",
      "F1 score: 0.4416431367807509\n",
      "\n",
      "-----Epoch 10/250-----\n",
      "Batch 150/485, loss: 0.5802235848704974 (0.492s)\n",
      "Batch 300/485, loss: 0.6568369186917941 (0.490s)\n",
      "Batch 450/485, loss: 0.5683001606663068 (0.490s)\n",
      "Batch 485/485, loss: 0.9324681375707898 (0.114s)\n",
      "F1 score: 0.4480255079996033\n",
      "\n",
      "-----Epoch 11/250-----\n",
      "Batch 150/485, loss: 0.5471715377767881 (0.492s)\n",
      "Batch 300/485, loss: 0.6010617387294769 (0.490s)\n",
      "Batch 450/485, loss: 0.6441325244307518 (0.490s)\n",
      "Batch 485/485, loss: 0.48538123411791667 (0.114s)\n",
      "F1 score: 0.454841938715851\n",
      "\n",
      "-----Epoch 12/250-----\n",
      "Batch 150/485, loss: 0.5335782259702683 (0.492s)\n",
      "Batch 300/485, loss: 0.6089461798469226 (0.490s)\n",
      "Batch 450/485, loss: 0.5650260078907013 (0.490s)\n",
      "Batch 485/485, loss: 0.5684606437172208 (0.114s)\n",
      "F1 score: 0.4652409928791039\n",
      "\n",
      "-----Epoch 13/250-----\n",
      "Batch 150/485, loss: 0.5885915686686833 (0.491s)\n",
      "Batch 300/485, loss: 0.58400072991848 (0.490s)\n",
      "Batch 450/485, loss: 0.5010058055321376 (0.489s)\n",
      "Batch 485/485, loss: 0.4056465583188193 (0.114s)\n",
      "F1 score: 0.4668019631748448\n",
      "\n",
      "-----Epoch 14/250-----\n",
      "Batch 150/485, loss: 0.5585709190368653 (0.492s)\n",
      "Batch 300/485, loss: 0.5311938732862472 (0.490s)\n",
      "Batch 450/485, loss: 0.49434386352698007 (0.489s)\n",
      "Batch 485/485, loss: 0.5940167808106968 (0.115s)\n",
      "F1 score: 0.47444706410197257\n",
      "\n",
      "-----Epoch 15/250-----\n",
      "Batch 150/485, loss: 0.4572975693643093 (0.492s)\n",
      "Batch 300/485, loss: 0.49957563360532126 (0.490s)\n",
      "Batch 450/485, loss: 0.5910027110079924 (0.489s)\n",
      "Batch 485/485, loss: 0.4682683506182262 (0.115s)\n",
      "F1 score: 0.4799090115647628\n",
      "\n",
      "-----Epoch 16/250-----\n",
      "Batch 150/485, loss: 0.4275925155977408 (0.492s)\n",
      "Batch 300/485, loss: 0.6326286914944649 (0.491s)\n",
      "Batch 450/485, loss: 0.5373490429421266 (0.489s)\n",
      "Batch 485/485, loss: 0.29638803218092236 (0.115s)\n",
      "F1 score: 0.4840650763998001\n",
      "\n",
      "-----Epoch 17/250-----\n",
      "Batch 150/485, loss: 0.5133831404149533 (0.491s)\n",
      "Batch 300/485, loss: 0.49523218964536986 (0.490s)\n",
      "Batch 450/485, loss: 0.4485156504313151 (0.490s)\n",
      "Batch 485/485, loss: 0.40371597324098857 (0.114s)\n",
      "F1 score: 0.4944975877358044\n",
      "\n",
      "-----Epoch 18/250-----\n",
      "Batch 150/485, loss: 0.428826614767313 (0.492s)\n",
      "Batch 300/485, loss: 0.5161347237726053 (0.491s)\n",
      "Batch 450/485, loss: 0.42915231655041375 (0.490s)\n",
      "Batch 485/485, loss: 0.52069859121527 (0.115s)\n",
      "F1 score: 0.49992824225076804\n",
      "\n",
      "-----Epoch 19/250-----\n",
      "Batch 150/485, loss: 0.453029375821352 (0.492s)\n",
      "Batch 300/485, loss: 0.4883536350230376 (0.490s)\n",
      "Batch 450/485, loss: 0.40742172141869865 (0.490s)\n",
      "Batch 485/485, loss: 0.37986933929579597 (0.114s)\n",
      "F1 score: 0.5087763962400776\n",
      "\n",
      "-----Epoch 20/250-----\n",
      "Batch 150/485, loss: 0.29455906569957735 (0.492s)\n",
      "Batch 300/485, loss: 0.5261316298941772 (0.490s)\n",
      "Batch 450/485, loss: 0.5025746572514375 (0.490s)\n",
      "Batch 485/485, loss: 0.27859286602054323 (0.115s)\n",
      "F1 score: 0.5164743032768577\n",
      "\n",
      "-----Epoch 21/250-----\n",
      "Batch 150/485, loss: 0.3980902113517125 (0.492s)\n",
      "Batch 300/485, loss: 0.454524167428414 (0.490s)\n",
      "Batch 450/485, loss: 0.38829882770776747 (0.490s)\n",
      "Batch 485/485, loss: 0.4553955444267818 (0.114s)\n",
      "F1 score: 0.5253693012524311\n",
      "\n",
      "-----Epoch 22/250-----\n",
      "Batch 150/485, loss: 0.4447383783757687 (0.491s)\n",
      "Batch 300/485, loss: 0.4098527730504672 (0.490s)\n",
      "Batch 450/485, loss: 0.4299151433010896 (0.489s)\n",
      "Batch 485/485, loss: 0.4917642689176968 (0.114s)\n",
      "F1 score: 0.5183733773217565\n",
      "\n",
      "-----Epoch 23/250-----\n",
      "Batch 150/485, loss: 0.39829447607199353 (0.491s)\n",
      "Batch 300/485, loss: 0.4097070150077343 (0.490s)\n",
      "Batch 450/485, loss: 0.43432528505722684 (0.492s)\n",
      "Batch 485/485, loss: 0.2676921574132783 (0.115s)\n",
      "F1 score: 0.5345598527396217\n",
      "\n",
      "-----Epoch 24/250-----\n",
      "Batch 150/485, loss: 0.45040669580300646 (0.494s)\n",
      "Batch 300/485, loss: 0.2920947320759296 (0.492s)\n",
      "Batch 450/485, loss: 0.4093866023917993 (0.491s)\n",
      "Batch 485/485, loss: 0.460875218468053 (0.115s)\n",
      "F1 score: 0.5471918628289174\n",
      "\n",
      "-----Epoch 25/250-----\n",
      "Batch 150/485, loss: 0.3968270915746689 (0.493s)\n",
      "Batch 300/485, loss: 0.3707064763704936 (0.491s)\n",
      "Batch 450/485, loss: 0.4256144406894843 (0.490s)\n",
      "Batch 485/485, loss: 0.21763020988021578 (0.115s)\n",
      "Saved to /home/hice1/khom9/CSE 8803 BMI Final Project/models/cnn_model-1.pth\n",
      "F1 score: 0.5523480865378955\n",
      "\n",
      "-----Epoch 26/250-----\n",
      "Batch 150/485, loss: 0.34104961678385737 (0.494s)\n",
      "Batch 300/485, loss: 0.3901742938657602 (0.491s)\n",
      "Batch 450/485, loss: 0.43460067038734757 (0.491s)\n",
      "Batch 485/485, loss: 0.4346185654401779 (0.115s)\n",
      "F1 score: 0.5519738163793259\n",
      "\n",
      "-----Epoch 27/250-----\n",
      "Batch 150/485, loss: 0.47269698590040204 (0.493s)\n",
      "Batch 300/485, loss: 0.305809512535731 (0.491s)\n",
      "Batch 450/485, loss: 0.42925444170832633 (0.490s)\n",
      "Batch 485/485, loss: 0.3132806196808815 (0.115s)\n",
      "F1 score: 0.5475441627732603\n",
      "\n",
      "-----Epoch 28/250-----\n",
      "Batch 150/485, loss: 0.3777944853901863 (0.493s)\n",
      "Batch 300/485, loss: 0.3117155014971892 (0.491s)\n",
      "Batch 450/485, loss: 0.4206843111415704 (0.490s)\n",
      "Batch 485/485, loss: 0.443460596033505 (0.115s)\n",
      "F1 score: 0.5694541886651582\n",
      "\n",
      "-----Epoch 29/250-----\n",
      "Batch 150/485, loss: 0.36466983025272687 (0.493s)\n",
      "Batch 300/485, loss: 0.3326428936918577 (0.491s)\n",
      "Batch 450/485, loss: 0.32445354099075 (0.490s)\n",
      "Batch 485/485, loss: 0.6085191366927964 (0.115s)\n",
      "F1 score: 0.580985127283809\n",
      "\n",
      "-----Epoch 30/250-----\n",
      "Batch 150/485, loss: 0.3989625438302755 (0.493s)\n",
      "Batch 300/485, loss: 0.32217394188046455 (0.491s)\n",
      "Batch 450/485, loss: 0.37385768607258796 (0.490s)\n",
      "Batch 485/485, loss: 0.32851604925734657 (0.115s)\n",
      "F1 score: 0.5805812809698679\n",
      "\n",
      "-----Epoch 31/250-----\n",
      "Batch 150/485, loss: 0.2908813586334387 (0.494s)\n",
      "Batch 300/485, loss: 0.4581542894740899 (0.491s)\n",
      "Batch 450/485, loss: 0.3326224495222171 (0.490s)\n",
      "Batch 485/485, loss: 0.2876821377447673 (0.115s)\n",
      "F1 score: 0.589129155526419\n",
      "\n",
      "-----Epoch 32/250-----\n",
      "Batch 150/485, loss: 0.2900440641740958 (0.493s)\n",
      "Batch 300/485, loss: 0.4898322073370218 (0.491s)\n",
      "Batch 450/485, loss: 0.3384686316549778 (0.490s)\n",
      "Batch 485/485, loss: 0.32368509556565966 (0.115s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.5795351748219533\n",
      "\n",
      "-----Epoch 33/250-----\n",
      "Batch 150/485, loss: 0.4150058332830667 (0.494s)\n",
      "Batch 300/485, loss: 0.33097900440295536 (0.491s)\n",
      "Batch 450/485, loss: 0.32575667060911656 (0.490s)\n",
      "Batch 485/485, loss: 0.38343204600470404 (0.114s)\n",
      "F1 score: 0.5897770519181216\n",
      "\n",
      "-----Epoch 34/250-----\n",
      "Batch 150/485, loss: 0.3095662558078766 (0.493s)\n",
      "Batch 300/485, loss: 0.35924288637936114 (0.491s)\n",
      "Batch 450/485, loss: 0.3295237411806981 (0.490s)\n",
      "Batch 485/485, loss: 0.3703146516212395 (0.114s)\n",
      "F1 score: 0.611008721540474\n",
      "\n",
      "-----Epoch 35/250-----\n",
      "Batch 150/485, loss: 0.31012739256024363 (0.492s)\n",
      "Batch 300/485, loss: 0.30384995403389137 (0.490s)\n",
      "Batch 450/485, loss: 0.31168015653888387 (0.490s)\n",
      "Batch 485/485, loss: 0.5687453356172357 (0.115s)\n",
      "F1 score: 0.6245788982668756\n",
      "\n",
      "-----Epoch 36/250-----\n",
      "Batch 150/485, loss: 0.36755525164306163 (0.493s)\n",
      "Batch 300/485, loss: 0.23648883566260337 (0.490s)\n",
      "Batch 450/485, loss: 0.35659697058300177 (0.491s)\n",
      "Batch 485/485, loss: 0.38842276600854736 (0.115s)\n",
      "F1 score: 0.6275521633661089\n",
      "\n",
      "-----Epoch 37/250-----\n",
      "Batch 150/485, loss: 0.38541441900034745 (0.493s)\n",
      "Batch 300/485, loss: 0.24524777901669342 (0.491s)\n",
      "Batch 450/485, loss: 0.29037578992545604 (0.491s)\n",
      "Batch 485/485, loss: 0.5163689535643373 (0.115s)\n",
      "F1 score: 0.631356453410448\n",
      "\n",
      "-----Epoch 38/250-----\n",
      "Batch 150/485, loss: 0.2901338122288386 (0.493s)\n",
      "Batch 300/485, loss: 0.39438396252691743 (0.491s)\n",
      "Batch 450/485, loss: 0.32090177103877066 (0.490s)\n",
      "Batch 485/485, loss: 0.2276502800839288 (0.115s)\n",
      "F1 score: 0.6289545971906882\n",
      "\n",
      "-----Epoch 39/250-----\n",
      "Batch 150/485, loss: 0.19889185746510823 (0.492s)\n",
      "Batch 300/485, loss: 0.389533583521843 (0.492s)\n",
      "Batch 450/485, loss: 0.3767770732442538 (0.491s)\n",
      "Batch 485/485, loss: 0.3660225474408695 (0.115s)\n",
      "F1 score: 0.6360682049273638\n",
      "\n",
      "-----Epoch 40/250-----\n",
      "Batch 150/485, loss: 0.3399134588241577 (0.493s)\n",
      "Batch 300/485, loss: 0.33548140831291673 (0.491s)\n",
      "Batch 450/485, loss: 0.2706425862014294 (0.491s)\n",
      "Batch 485/485, loss: 0.3802656990076814 (0.115s)\n",
      "F1 score: 0.6472573107123651\n",
      "\n",
      "-----Epoch 41/250-----\n",
      "Batch 150/485, loss: 0.29963341074685257 (0.493s)\n",
      "Batch 300/485, loss: 0.3498456217845281 (0.491s)\n",
      "Batch 450/485, loss: 0.35808683986465134 (0.490s)\n",
      "Batch 485/485, loss: 0.1942540881889207 (0.115s)\n",
      "F1 score: 0.6386050553736962\n",
      "\n",
      "-----Epoch 42/250-----\n",
      "Batch 150/485, loss: 0.28928642041981223 (0.493s)\n",
      "Batch 300/485, loss: 0.2818904630343119 (0.491s)\n",
      "Batch 450/485, loss: 0.3188898002604644 (0.491s)\n",
      "Batch 485/485, loss: 0.5595228402742318 (0.115s)\n",
      "F1 score: 0.6482417269653705\n",
      "\n",
      "-----Epoch 43/250-----\n",
      "Batch 150/485, loss: 0.36463889315724374 (0.492s)\n",
      "Batch 300/485, loss: 0.3093618912498156 (0.491s)\n",
      "Batch 450/485, loss: 0.27931522369384765 (0.490s)\n",
      "Batch 485/485, loss: 0.26575027502008847 (0.115s)\n",
      "F1 score: 0.658340971467539\n",
      "\n",
      "-----Epoch 44/250-----\n",
      "Batch 150/485, loss: 0.27761263844867545 (0.492s)\n",
      "Batch 300/485, loss: 0.21292239936689536 (0.491s)\n",
      "Batch 450/485, loss: 0.4400702359775702 (0.490s)\n",
      "Batch 485/485, loss: 0.3907217219471931 (0.115s)\n",
      "F1 score: 0.6587212782338154\n",
      "\n",
      "-----Epoch 45/250-----\n",
      "Batch 150/485, loss: 0.32851267139116924 (0.493s)\n",
      "Batch 300/485, loss: 0.356647282615304 (0.491s)\n",
      "Batch 450/485, loss: 0.26886457997063795 (0.491s)\n",
      "Batch 485/485, loss: 0.2799926179860319 (0.115s)\n",
      "F1 score: 0.6547316731227523\n",
      "\n",
      "-----Epoch 46/250-----\n",
      "Batch 150/485, loss: 0.21657307510574658 (0.494s)\n",
      "Batch 300/485, loss: 0.3571623474359512 (0.492s)\n",
      "Batch 450/485, loss: 0.4117342404772838 (0.493s)\n",
      "Batch 485/485, loss: 0.2680492138223989 (0.116s)\n",
      "F1 score: 0.6584535849675753\n",
      "\n",
      "-----Epoch 47/250-----\n",
      "Batch 150/485, loss: 0.32342107343177 (0.495s)\n",
      "Batch 300/485, loss: 0.3668903813759486 (0.493s)\n",
      "Batch 450/485, loss: 0.243584726502498 (0.493s)\n",
      "Batch 485/485, loss: 0.4053774927343641 (0.115s)\n",
      "F1 score: 0.6525530547230226\n",
      "\n",
      "-----Epoch 48/250-----\n",
      "Batch 150/485, loss: 0.24718961680928866 (0.496s)\n",
      "Batch 300/485, loss: 0.3217433956762155 (0.494s)\n",
      "Batch 450/485, loss: 0.35343240829805533 (0.492s)\n",
      "Batch 485/485, loss: 0.26437204756907057 (0.115s)\n",
      "F1 score: 0.6779226859661835\n",
      "\n",
      "-----Epoch 49/250-----\n",
      "Batch 150/485, loss: 0.317253757417202 (0.496s)\n",
      "Batch 300/485, loss: 0.23987214602530002 (0.493s)\n",
      "Batch 450/485, loss: 0.32731571483115357 (0.493s)\n",
      "Batch 485/485, loss: 0.4048823264028345 (0.115s)\n",
      "F1 score: 0.683526735562161\n",
      "\n",
      "-----Epoch 50/250-----\n",
      "Batch 150/485, loss: 0.28001700242360433 (0.495s)\n",
      "Batch 300/485, loss: 0.3602989687770605 (0.493s)\n",
      "Batch 450/485, loss: 0.28835886421302953 (0.493s)\n",
      "Batch 485/485, loss: 0.30620552812303814 (0.115s)\n",
      "Saved to /home/hice1/khom9/CSE 8803 BMI Final Project/models/cnn_model-1.pth\n",
      "F1 score: 0.6725609581119963\n",
      "\n",
      "-----Epoch 51/250-----\n",
      "Batch 150/485, loss: 0.2825941619773706 (0.496s)\n",
      "Batch 300/485, loss: 0.3169664544115464 (0.494s)\n",
      "Batch 450/485, loss: 0.27116079171498614 (0.493s)\n",
      "Batch 485/485, loss: 0.4423274055123329 (0.115s)\n",
      "F1 score: 0.6927340818072777\n",
      "\n",
      "-----Epoch 52/250-----\n",
      "Batch 150/485, loss: 0.266639662583669 (0.494s)\n",
      "Batch 300/485, loss: 0.29634641023973624 (0.493s)\n",
      "Batch 450/485, loss: 0.3073476133743922 (0.492s)\n",
      "Batch 485/485, loss: 0.33161360025405884 (0.115s)\n",
      "F1 score: 0.713264714546939\n",
      "\n",
      "-----Epoch 53/250-----\n",
      "Batch 150/485, loss: 0.3039593676105142 (0.495s)\n",
      "Batch 300/485, loss: 0.24879574423034986 (0.492s)\n",
      "Batch 450/485, loss: 0.2954044754058123 (0.491s)\n",
      "Batch 485/485, loss: 0.3929762799292803 (0.115s)\n",
      "F1 score: 0.7214766907650785\n",
      "\n",
      "-----Epoch 54/250-----\n",
      "Batch 150/485, loss: 0.25634796895086764 (0.493s)\n",
      "Batch 300/485, loss: 0.3349841708689928 (0.493s)\n",
      "Batch 450/485, loss: 0.26815168641507625 (0.492s)\n",
      "Batch 485/485, loss: 0.31902058092611174 (0.115s)\n",
      "F1 score: 0.728697170514708\n",
      "\n",
      "-----Epoch 55/250-----\n",
      "Batch 150/485, loss: 0.2651830618456006 (0.493s)\n",
      "Batch 300/485, loss: 0.3140941381206115 (0.493s)\n",
      "Batch 450/485, loss: 0.29782625174770755 (0.491s)\n",
      "Batch 485/485, loss: 0.2783954783209733 (0.115s)\n",
      "F1 score: 0.723981816416869\n",
      "\n",
      "-----Epoch 56/250-----\n",
      "Batch 150/485, loss: 0.3407339443763097 (0.494s)\n",
      "Batch 300/485, loss: 0.22133343933771055 (0.492s)\n",
      "Batch 450/485, loss: 0.3477348915860057 (0.490s)\n",
      "Batch 485/485, loss: 0.08856814045991217 (0.114s)\n",
      "F1 score: 0.7344380631656223\n",
      "\n",
      "-----Epoch 57/250-----\n",
      "Batch 150/485, loss: 0.31497616271177925 (0.494s)\n",
      "Batch 300/485, loss: 0.334361292347312 (0.491s)\n",
      "Batch 450/485, loss: 0.2529545677577456 (0.491s)\n",
      "Batch 485/485, loss: 0.10471430484737669 (0.115s)\n",
      "F1 score: 0.7391140769921082\n",
      "\n",
      "-----Epoch 58/250-----\n",
      "Batch 150/485, loss: 0.2521375721941392 (0.494s)\n",
      "Batch 300/485, loss: 0.29420883012314636 (0.492s)\n",
      "Batch 450/485, loss: 0.3489327125872175 (0.490s)\n",
      "Batch 485/485, loss: 0.1647592504109655 (0.114s)\n",
      "F1 score: 0.737295577422248\n",
      "\n",
      "-----Epoch 59/250-----\n",
      "Batch 150/485, loss: 0.31357407197356224 (0.495s)\n",
      "Batch 300/485, loss: 0.3271259537835916 (0.491s)\n",
      "Batch 450/485, loss: 0.262581165942053 (0.491s)\n",
      "Batch 485/485, loss: 0.21436610812587398 (0.115s)\n",
      "F1 score: 0.718687530386665\n",
      "\n",
      "-----Epoch 60/250-----\n",
      "Batch 150/485, loss: 0.2674423940976461 (0.493s)\n",
      "Batch 300/485, loss: 0.29375450080881516 (0.491s)\n",
      "Batch 450/485, loss: 0.3156284917270144 (0.491s)\n",
      "Batch 485/485, loss: 0.18865679635533264 (0.115s)\n",
      "F1 score: 0.7473705269831452\n",
      "\n",
      "-----Epoch 61/250-----\n",
      "Batch 150/485, loss: 0.2276972945034504 (0.494s)\n",
      "Batch 300/485, loss: 0.3200153994187713 (0.492s)\n",
      "Batch 450/485, loss: 0.3161136612916986 (0.491s)\n",
      "Batch 485/485, loss: 0.22056490975831236 (0.115s)\n",
      "F1 score: 0.7523417458952877\n",
      "\n",
      "-----Epoch 62/250-----\n",
      "Batch 150/485, loss: 0.17790016087392965 (0.493s)\n",
      "Batch 300/485, loss: 0.3719689718882243 (0.491s)\n",
      "Batch 450/485, loss: 0.26598279893398286 (0.490s)\n",
      "Batch 485/485, loss: 0.42473448419145177 (0.115s)\n",
      "F1 score: 0.7553802150410631\n",
      "\n",
      "-----Epoch 63/250-----\n",
      "Batch 150/485, loss: 0.3155822572360436 (0.493s)\n",
      "Batch 300/485, loss: 0.31414303958415984 (0.492s)\n",
      "Batch 450/485, loss: 0.22774608340114355 (0.491s)\n",
      "Batch 485/485, loss: 0.29617679720478396 (0.115s)\n",
      "F1 score: 0.7471199381895607\n",
      "\n",
      "-----Epoch 64/250-----\n",
      "Batch 150/485, loss: 0.2814586015666525 (0.493s)\n",
      "Batch 300/485, loss: 0.2611996093764901 (0.491s)\n",
      "Batch 450/485, loss: 0.26995473631968103 (0.491s)\n",
      "Batch 485/485, loss: 0.4654977519597326 (0.115s)\n",
      "F1 score: 0.7556259845686093\n",
      "\n",
      "-----Epoch 65/250-----\n",
      "Batch 150/485, loss: 0.30533495972553887 (0.494s)\n",
      "Batch 300/485, loss: 0.235937981903553 (0.491s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 450/485, loss: 0.3073887616644303 (0.491s)\n",
      "Batch 485/485, loss: 0.22723393477499484 (0.115s)\n",
      "F1 score: 0.7683184917451736\n",
      "\n",
      "-----Epoch 66/250-----\n",
      "Batch 150/485, loss: 0.3831789926439524 (0.494s)\n",
      "Batch 300/485, loss: 0.18703221843888362 (0.492s)\n",
      "Batch 450/485, loss: 0.2835490345209837 (0.492s)\n",
      "Batch 485/485, loss: 0.18947035553199904 (0.115s)\n",
      "F1 score: 0.7743136516336099\n",
      "\n",
      "-----Epoch 67/250-----\n",
      "Batch 150/485, loss: 0.2747685305898388 (0.494s)\n",
      "Batch 300/485, loss: 0.22395578128596147 (0.491s)\n",
      "Batch 450/485, loss: 0.3393317119901379 (0.491s)\n",
      "Batch 485/485, loss: 0.29482312122625964 (0.115s)\n",
      "F1 score: 0.7673962488685404\n",
      "\n",
      "-----Epoch 68/250-----\n",
      "Batch 150/485, loss: 0.3358397975564003 (0.494s)\n",
      "Batch 300/485, loss: 0.2390079396838943 (0.492s)\n",
      "Batch 450/485, loss: 0.2671579742555817 (0.492s)\n",
      "Batch 485/485, loss: 0.21001040350113595 (0.116s)\n",
      "F1 score: 0.7827009139104966\n",
      "\n",
      "-----Epoch 69/250-----\n",
      "Batch 150/485, loss: 0.23379149213433265 (0.493s)\n",
      "Batch 300/485, loss: 0.27271334127833446 (0.492s)\n",
      "Batch 450/485, loss: 0.3214246211325129 (0.491s)\n",
      "Batch 485/485, loss: 0.3376467097018446 (0.115s)\n",
      "F1 score: 0.7758788574607135\n",
      "\n",
      "-----Epoch 70/250-----\n",
      "Batch 150/485, loss: 0.2409879683579008 (0.493s)\n",
      "Batch 300/485, loss: 0.29106936141848566 (0.492s)\n",
      "Batch 450/485, loss: 0.3334558713187774 (0.492s)\n",
      "Batch 485/485, loss: 0.09263842249555247 (0.115s)\n",
      "F1 score: 0.7904080244681354\n",
      "\n",
      "-----Epoch 71/250-----\n",
      "Batch 150/485, loss: 0.25931208197027444 (0.494s)\n",
      "Batch 300/485, loss: 0.253786254140238 (0.492s)\n",
      "Batch 450/485, loss: 0.3143387487779061 (0.491s)\n",
      "Batch 485/485, loss: 0.2978252400244985 (0.115s)\n",
      "F1 score: 0.7814608589987319\n",
      "\n",
      "-----Epoch 72/250-----\n",
      "Batch 150/485, loss: 0.21059292297810317 (0.494s)\n",
      "Batch 300/485, loss: 0.3847370951250195 (0.492s)\n",
      "Batch 450/485, loss: 0.21653687622398138 (0.492s)\n",
      "Batch 485/485, loss: 0.29578098697321753 (0.115s)\n",
      "F1 score: 0.7977715697824723\n",
      "\n",
      "-----Epoch 73/250-----\n",
      "Batch 150/485, loss: 0.24141322133441767 (0.495s)\n",
      "Batch 300/485, loss: 0.2324689352636536 (0.492s)\n",
      "Batch 450/485, loss: 0.3743912569681803 (0.492s)\n",
      "Batch 485/485, loss: 0.28956062900168555 (0.115s)\n",
      "F1 score: 0.7852771698250088\n",
      "\n",
      "-----Epoch 74/250-----\n",
      "Batch 150/485, loss: 0.2660439005245765 (0.494s)\n",
      "Batch 300/485, loss: 0.3444937257096171 (0.492s)\n",
      "Batch 450/485, loss: 0.26301783123364053 (0.492s)\n",
      "Batch 485/485, loss: 0.061949763926012175 (0.115s)\n",
      "F1 score: 0.786006606488503\n",
      "\n",
      "-----Epoch 75/250-----\n",
      "Batch 150/485, loss: 0.17177055011192957 (0.494s)\n",
      "Batch 300/485, loss: 0.25621445551514627 (0.491s)\n",
      "Batch 450/485, loss: 0.3627504551038146 (0.491s)\n",
      "Batch 485/485, loss: 0.36348008806152005 (0.115s)\n",
      "Saved to /home/hice1/khom9/CSE 8803 BMI Final Project/models/cnn_model-1.pth\n",
      "F1 score: 0.8053620937619985\n",
      "\n",
      "-----Epoch 76/250-----\n",
      "Batch 150/485, loss: 0.2588353866835435 (0.495s)\n",
      "Batch 300/485, loss: 0.24154493084798256 (0.492s)\n",
      "Batch 450/485, loss: 0.36458662966887156 (0.491s)\n",
      "Batch 485/485, loss: 0.09027003209505763 (0.115s)\n",
      "F1 score: 0.7922006013310253\n",
      "\n",
      "-----Epoch 77/250-----\n",
      "Batch 150/485, loss: 0.2937526205802957 (0.494s)\n",
      "Batch 300/485, loss: 0.29285340547561645 (0.492s)\n",
      "Batch 450/485, loss: 0.22899642566839853 (0.492s)\n",
      "Batch 485/485, loss: 0.2507904263479369 (0.115s)\n",
      "F1 score: 0.8061612710565281\n",
      "\n",
      "-----Epoch 78/250-----\n",
      "Batch 150/485, loss: 0.289204807455341 (0.494s)\n",
      "Batch 300/485, loss: 0.19209824410577614 (0.491s)\n",
      "Batch 450/485, loss: 0.3322541107982397 (0.490s)\n",
      "Batch 485/485, loss: 0.35272146144083566 (0.115s)\n",
      "F1 score: 0.7911703540333838\n",
      "\n",
      "-----Epoch 79/250-----\n",
      "Batch 150/485, loss: 0.19613399708022675 (0.493s)\n",
      "Batch 300/485, loss: 0.3111377169812719 (0.491s)\n",
      "Batch 450/485, loss: 0.28111141089349984 (0.491s)\n",
      "Batch 485/485, loss: 0.4136186036680426 (0.115s)\n",
      "F1 score: 0.7942365477589693\n",
      "\n",
      "-----Epoch 80/250-----\n",
      "Batch 150/485, loss: 0.17382458593696357 (0.494s)\n",
      "Batch 300/485, loss: 0.30893663292129836 (0.492s)\n",
      "Batch 450/485, loss: 0.3171513738979896 (0.491s)\n",
      "Batch 485/485, loss: 0.4185152120888233 (0.114s)\n",
      "F1 score: 0.805215658173302\n",
      "\n",
      "-----Epoch 81/250-----\n",
      "Batch 150/485, loss: 0.247763696834445 (0.494s)\n",
      "Batch 300/485, loss: 0.25239497020840646 (0.492s)\n",
      "Batch 450/485, loss: 0.3352164342502753 (0.491s)\n",
      "Batch 485/485, loss: 0.19830782482666628 (0.115s)\n",
      "F1 score: 0.7960333694056555\n",
      "\n",
      "-----Epoch 82/250-----\n",
      "Batch 150/485, loss: 0.27199730431040126 (0.495s)\n",
      "Batch 300/485, loss: 0.2190915673598647 (0.492s)\n",
      "Batch 450/485, loss: 0.32200104527175427 (0.492s)\n",
      "Batch 485/485, loss: 0.2653559376086507 (0.115s)\n",
      "F1 score: 0.8114242525764926\n",
      "\n",
      "-----Epoch 83/250-----\n",
      "Batch 150/485, loss: 0.3519407955557108 (0.494s)\n",
      "Batch 300/485, loss: 0.2179947472239534 (0.490s)\n",
      "Batch 450/485, loss: 0.2722216453651587 (0.491s)\n",
      "Batch 485/485, loss: 0.15578563032405718 (0.115s)\n",
      "F1 score: 0.802522502478986\n",
      "\n",
      "-----Epoch 84/250-----\n",
      "Batch 150/485, loss: 0.26053666602199277 (0.494s)\n",
      "Batch 300/485, loss: 0.37825623581806816 (0.492s)\n",
      "Batch 450/485, loss: 0.195670998506248 (0.491s)\n",
      "Batch 485/485, loss: 0.13299997305231434 (0.115s)\n",
      "F1 score: 0.8187743364380863\n",
      "\n",
      "-----Epoch 85/250-----\n",
      "Batch 150/485, loss: 0.27000163480639455 (0.495s)\n",
      "Batch 300/485, loss: 0.27406594982991617 (0.493s)\n",
      "Batch 450/485, loss: 0.24331265478084485 (0.492s)\n",
      "Batch 485/485, loss: 0.31569111049175264 (0.115s)\n",
      "F1 score: 0.8244676068355481\n",
      "\n",
      "-----Epoch 86/250-----\n",
      "Batch 150/485, loss: 0.26481819612905383 (0.494s)\n",
      "Batch 300/485, loss: 0.3145495593858262 (0.493s)\n",
      "Batch 450/485, loss: 0.20841215488811335 (0.491s)\n",
      "Batch 485/485, loss: 0.3216964457184076 (0.115s)\n",
      "F1 score: 0.8244734743291201\n",
      "\n",
      "-----Epoch 87/250-----\n",
      "Batch 150/485, loss: 0.3512902458446721 (0.496s)\n",
      "Batch 300/485, loss: 0.23518489961822828 (0.496s)\n",
      "Batch 450/485, loss: 0.23376871803154547 (0.491s)\n",
      "Batch 485/485, loss: 0.15697262159415654 (0.115s)\n",
      "F1 score: 0.8315534527971622\n",
      "\n",
      "-----Epoch 88/250-----\n",
      "Batch 150/485, loss: 0.19776607195536294 (0.493s)\n",
      "Batch 300/485, loss: 0.22782312611738842 (0.492s)\n",
      "Batch 450/485, loss: 0.30217097208524746 (0.491s)\n",
      "Batch 485/485, loss: 0.5375232800309148 (0.115s)\n",
      "F1 score: 0.8390331262095503\n",
      "\n",
      "-----Epoch 89/250-----\n",
      "Batch 150/485, loss: 0.26338012975951036 (0.492s)\n",
      "Batch 300/485, loss: 0.273763214237988 (0.491s)\n",
      "Batch 450/485, loss: 0.2521499693890413 (0.489s)\n",
      "Batch 485/485, loss: 0.27744731727455346 (0.114s)\n",
      "F1 score: 0.8377065154198317\n",
      "\n",
      "-----Epoch 90/250-----\n",
      "Batch 150/485, loss: 0.23673407330488166 (0.491s)\n",
      "Batch 300/485, loss: 0.31812745016689103 (0.489s)\n",
      "Batch 450/485, loss: 0.21560729417949914 (0.489s)\n",
      "Batch 485/485, loss: 0.370687342860869 (0.114s)\n",
      "F1 score: 0.8387959367362566\n",
      "\n",
      "-----Epoch 91/250-----\n",
      "Batch 150/485, loss: 0.21707725253577034 (0.492s)\n",
      "Batch 300/485, loss: 0.2501315165311098 (0.490s)\n",
      "Batch 450/485, loss: 0.3440737000728647 (0.489s)\n",
      "Batch 485/485, loss: 0.3325519951858691 (0.114s)\n",
      "F1 score: 0.8193306341395196\n",
      "\n",
      "-----Epoch 92/250-----\n",
      "Batch 150/485, loss: 0.2710557898754875 (0.493s)\n",
      "Batch 300/485, loss: 0.36238693342233697 (0.490s)\n",
      "Batch 450/485, loss: 0.19987395452956358 (0.488s)\n",
      "Batch 485/485, loss: 0.12064038190458502 (0.114s)\n",
      "F1 score: 0.8293675253498164\n",
      "\n",
      "-----Epoch 93/250-----\n",
      "Batch 150/485, loss: 0.26302573005358376 (0.492s)\n",
      "Batch 300/485, loss: 0.27220189516743026 (0.490s)\n",
      "Batch 450/485, loss: 0.2525138408690691 (0.489s)\n",
      "Batch 485/485, loss: 0.30497451701334544 (0.114s)\n",
      "F1 score: 0.8406523842976414\n",
      "\n",
      "-----Epoch 94/250-----\n",
      "Batch 150/485, loss: 0.1993885478625695 (0.491s)\n",
      "Batch 300/485, loss: 0.27303276749327776 (0.489s)\n",
      "Batch 450/485, loss: 0.29898286911969385 (0.489s)\n",
      "Batch 485/485, loss: 0.34790185556880066 (0.115s)\n",
      "F1 score: 0.846686427510402\n",
      "\n",
      "-----Epoch 95/250-----\n",
      "Batch 150/485, loss: 0.24192328151936332 (0.492s)\n",
      "Batch 300/485, loss: 0.27298888214553396 (0.490s)\n",
      "Batch 450/485, loss: 0.20667280675222477 (0.489s)\n",
      "Batch 485/485, loss: 0.550289905497006 (0.114s)\n",
      "F1 score: 0.8480987938424869\n",
      "\n",
      "-----Epoch 96/250-----\n",
      "Batch 150/485, loss: 0.245477432521681 (0.491s)\n",
      "Batch 300/485, loss: 0.3601861467212439 (0.490s)\n",
      "Batch 450/485, loss: 0.21275168906276426 (0.488s)\n",
      "Batch 485/485, loss: 0.1188694021797606 (0.114s)\n",
      "F1 score: 0.8545726742091085\n",
      "\n",
      "-----Epoch 97/250-----\n",
      "Batch 150/485, loss: 0.2725805855480333 (0.492s)\n",
      "Batch 300/485, loss: 0.27716912117476267 (0.489s)\n",
      "Batch 450/485, loss: 0.19487204444905123 (0.489s)\n",
      "Batch 485/485, loss: 0.511375953150647 (0.114s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.8574429296163414\n",
      "\n",
      "-----Epoch 98/250-----\n",
      "Batch 150/485, loss: 0.23259851648161808 (0.491s)\n",
      "Batch 300/485, loss: 0.34695525697122015 (0.490s)\n",
      "Batch 450/485, loss: 0.2263740887803336 (0.489s)\n",
      "Batch 485/485, loss: 0.23940836143280778 (0.114s)\n",
      "F1 score: 0.8415471260239781\n",
      "\n",
      "-----Epoch 99/250-----\n",
      "Batch 150/485, loss: 0.20237840236475071 (0.491s)\n",
      "Batch 300/485, loss: 0.2687944476430615 (0.490s)\n",
      "Batch 450/485, loss: 0.3495612770753602 (0.489s)\n",
      "Batch 485/485, loss: 0.15440324920096568 (0.114s)\n"
     ]
    }
   ],
   "source": [
    "epochs = 250\n",
    "train_model(model, optimizer, d, loss_fn, epochs=epochs, batch_size=batch_size, save_freq=25, \n",
    "            save_path=save_path, scheduler=scheduler, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6ef550",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(save_path, model, optimizer, epochs)\n",
    "print(f'Saved to {save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a0c335",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 8721 #18622\n",
    "txt, label = d[i]\n",
    "label = label.unsqueeze(0)\n",
    "print(loss_fn(model(txt.to(DEVICE)).detach(), label.to(DEVICE)).item())\n",
    "print(torch.cat([model(txt.to(DEVICE)).detach(), label.to(DEVICE)]).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36ea3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = (torch.sigmoid(batch_predict(model, d.abst_data, device=DEVICE).detach().cpu()) > 0.5).type(torch.float)\n",
    "true = d.labels.to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831c7a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = []\n",
    "loss_fn_cpu = loss_fn.cpu()\n",
    "for i in range(len(d)):\n",
    "    total_loss.append(loss_fn_cpu(pred[i].unsqueeze(0), true[i].unsqueeze(0)).item())\n",
    "    \n",
    "print(f'Total avg loss: {np.mean(total_loss)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f669161",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.Series(total_loss)\n",
    "x.sort_values(ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d865e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(precision_score(true, pred, average=None))\n",
    "print(f'Total precision: {precision_score(true, pred, average=\"weighted\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f164fc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recall_score(true, pred, average=None))\n",
    "print(f'Total recall: {recall_score(true, pred, average=\"weighted\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a9f732",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(true, pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851d848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total accuracy: {accuracy_score(true, pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aec5fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_tuned = gensim.models.KeyedVectors.load(EMBED_KEYS_PATH, mmap='r')\n",
    "wv_tuned.vectors = model.embedding.weight.data.detach().cpu().numpy()\n",
    "\n",
    "wv_tuned.vectors = np.clip(wv_tuned.vectors, a_min=0, a_max=1.)\n",
    "\n",
    "wv_tuned.save(wv_out_path)\n",
    "print(f'Saved word embeddings to {wv_out_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6160c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for txt, label in (d):\n",
    "        outputs = list(model(txt.to(DEVICE), all_outputs_max=True))\n",
    "        activations.append(outputs)\n",
    "activations = torch.tensor(activations)\n",
    "\n",
    "max_act = torch.max(activations, axis=0)[0]\n",
    "max_act_dict = OrderedDict(zip(list(dict(model.named_modules()).keys())[1:], max_act))\n",
    "\n",
    "output = open(act_path, 'wb')\n",
    "pickle.dump(max_act_dict, output)\n",
    "output.close()\n",
    "print(f'Wrote max layer activations to {act_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226a1b54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
