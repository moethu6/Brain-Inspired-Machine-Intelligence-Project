{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4db4fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "549609f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import OrderedDict\n",
    "from torch import nn, optim\n",
    "from torchtext.data import utils\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbf3b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import AbstractDataset\n",
    "from util import save_model, load_model_and_opt, batch_predict\n",
    "from cnn_util import CNNBase1, train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ef00680",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = (f'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "HOME = '/home/hice1/khom9/CSE 8803 BMI Final Project'\n",
    "EMBED_KEYS_PATH = f'{HOME}/wordvectors/abstracts200_normalized.wordvectors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f73b66aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type of CNN (1, 2, or 3); refer to cnn_util.py for more info\n",
    "VERSION = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d11360d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n",
      "NVIDIA L40S\n"
     ]
    }
   ],
   "source": [
    "print(f'Using device {DEVICE}')\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9b06e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf = pd.read_csv('CleanedAVdata.csv')\\nnltk.download('wordnet')\\ntk = utils.get_tokenizer('spacy')\\nlemma = WordNetLemmatizer()\\nabstracts = df['Abstract']\\nembed_dim = 200\\ntokens = pd.Series([[lemma.lemmatize(w) for w in tk(abst)] for abst in abstracts])\\nmodel = Word2Vec(sentences=tokens, vector_size=embed_dim, window=5, min_count=1, workers=12)\\n\\nmu = np.mean(model.wv.vectors)\\nsigma = np.sqrt(np.var(model.wv.vectors))\\nmodel.wv.vectors = (1 + (np.clip(model.wv.vectors, mu-3*sigma, mu+3*sigma) - mu) / (3*sigma)) / 2\\nmodel.wv.vectors = np.clip(model.wv.vectors, 0, 1)\\n\\nmodel.wv['\\x00'] = np.zeros(embed_dim)\\nmodel.wv.save(EMBED_KEYS_PATH)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only run once\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "'''\n",
    "df = pd.read_csv('CleanedAVdata.csv')\n",
    "nltk.download('wordnet')\n",
    "tk = utils.get_tokenizer('spacy')\n",
    "lemma = WordNetLemmatizer()\n",
    "abstracts = df['Abstract']\n",
    "embed_dim = 200\n",
    "tokens = pd.Series([[lemma.lemmatize(w) for w in tk(abst)] for abst in abstracts])\n",
    "model = Word2Vec(sentences=tokens, vector_size=embed_dim, window=5, min_count=1, workers=12)\n",
    "\n",
    "mu = np.mean(model.wv.vectors)\n",
    "sigma = np.sqrt(np.var(model.wv.vectors))\n",
    "model.wv.vectors = (1 + (np.clip(model.wv.vectors, mu-3*sigma, mu+3*sigma) - mu) / (3*sigma)) / 2\n",
    "model.wv.vectors = np.clip(model.wv.vectors, 0, 1)\n",
    "\n",
    "model.wv['\\0'] = np.zeros(embed_dim)\n",
    "model.wv.save(EMBED_KEYS_PATH)\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8654b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(\n",
      "100%|██████████| 23250/23250 [00:15<00:00, 1497.28it/s]\n"
     ]
    }
   ],
   "source": [
    "tk = utils.get_tokenizer('spacy')\n",
    "wv = gensim.models.KeyedVectors.load(EMBED_KEYS_PATH, mmap='r')\n",
    "null_word = '\\0'\n",
    "d = AbstractDataset('CleanedAVdata.csv', 'Abstract', 'IPCR Classifications', tk, wv.key_to_index,\n",
    "                    null_word=null_word, min_len=30, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fa7bf00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "batch_size = 48\n",
    "model = CNNBase1(EMBED_KEYS_PATH, null_word=null_word).to(DEVICE)\n",
    "\n",
    "save_path = f'{HOME}/models/cnn_model-{VERSION}.pth'\n",
    "act_path = f'{HOME}/models/cnn_model-{VERSION}-max-activations.pkl'\n",
    "wv_out_path = f'{HOME}/wordvectors/abstracts200_trained_normalized_{VERSION}.wordvectors'\n",
    "\n",
    "num_pos = d.labels.sum(axis=0, keepdim=True).to_dense()\n",
    "pos_weight = (d.labels.shape[0] - num_pos) / num_pos\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(DEVICE))\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# model, optimizer = load_model_and_opt(model, optimizer, save_path)\n",
    "\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffe5e3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.001\n",
      "Scheduler: <torch.optim.lr_scheduler.StepLR object at 0x15542eaef580>\n",
      "Training for 250 epochs, with batch size=48\n",
      "Using device: cuda:0\n",
      "Saving model every 25 epochs to /home/hice1/khom9/CSE 8803 BMI Final Project/models/cnn_model-1.pth\n",
      "\n",
      "-----Epoch 1/250-----\n",
      "Batch 150/485, loss: 1.407383065422376 (0.896s)\n",
      "Batch 300/485, loss: 1.3677325503031412 (0.497s)\n",
      "Batch 450/485, loss: 1.5280050901571909 (0.498s)\n",
      "Batch 485/485, loss: 1.2095337101391384 (0.124s)\n",
      "F1 score: 0.3848852972334508\n",
      "\n",
      "-----Epoch 2/250-----\n",
      "Batch 150/485, loss: 1.4273061811923982 (0.497s)\n",
      "Batch 300/485, loss: 1.2728089169661203 (0.494s)\n",
      "Batch 450/485, loss: 1.3472783998648326 (0.494s)\n",
      "Batch 485/485, loss: 1.3144826429230827 (0.116s)\n",
      "F1 score: 0.4018547505709647\n",
      "\n",
      "-----Epoch 3/250-----\n",
      "Batch 150/485, loss: 1.1586296979586284 (0.495s)\n",
      "Batch 300/485, loss: 1.0808591693639755 (0.492s)\n",
      "Batch 450/485, loss: 1.3127974275747936 (0.492s)\n",
      "Batch 485/485, loss: 1.0964189972196305 (0.115s)\n",
      "F1 score: 0.4317589272078272\n",
      "\n",
      "-----Epoch 4/250-----\n",
      "Batch 150/485, loss: 1.1200774125258128 (0.494s)\n",
      "Batch 300/485, loss: 1.0848438105980556 (0.491s)\n",
      "Batch 450/485, loss: 0.9091748962799708 (0.492s)\n",
      "Batch 485/485, loss: 1.0273271688393184 (0.115s)\n",
      "F1 score: 0.43874186252837405\n",
      "\n",
      "-----Epoch 5/250-----\n",
      "Batch 150/485, loss: 0.9306613536675771 (0.493s)\n",
      "Batch 300/485, loss: 0.9903192265828451 (0.492s)\n",
      "Batch 450/485, loss: 0.9079951250553131 (0.491s)\n",
      "Batch 485/485, loss: 1.1842296540737152 (0.115s)\n",
      "F1 score: 0.4326302065357958\n",
      "\n",
      "-----Epoch 6/250-----\n",
      "Batch 150/485, loss: 0.9603536271055539 (0.492s)\n",
      "Batch 300/485, loss: 0.8424507025877634 (0.491s)\n",
      "Batch 450/485, loss: 0.8705880673726399 (0.490s)\n",
      "Batch 485/485, loss: 0.7638850365366254 (0.115s)\n",
      "F1 score: 0.4319727544855964\n",
      "\n",
      "-----Epoch 7/250-----\n",
      "Batch 150/485, loss: 0.7655828142166138 (0.492s)\n",
      "Batch 300/485, loss: 0.8101103898882865 (0.490s)\n",
      "Batch 450/485, loss: 0.8443632664283117 (0.490s)\n",
      "Batch 485/485, loss: 0.6486113739865167 (0.115s)\n",
      "F1 score: 0.4301276630466317\n",
      "\n",
      "-----Epoch 8/250-----\n",
      "Batch 150/485, loss: 0.7437668852011363 (0.492s)\n",
      "Batch 300/485, loss: 0.6544478883345922 (0.490s)\n",
      "Batch 450/485, loss: 0.8074936825037002 (0.490s)\n",
      "Batch 485/485, loss: 0.6379450125353677 (0.115s)\n",
      "F1 score: 0.43550647920144225\n",
      "\n",
      "-----Epoch 9/250-----\n",
      "Batch 150/485, loss: 0.6808750801285108 (0.492s)\n",
      "Batch 300/485, loss: 0.6383366122841835 (0.491s)\n",
      "Batch 450/485, loss: 0.6759947408239046 (0.490s)\n",
      "Batch 485/485, loss: 0.742475808944021 (0.115s)\n",
      "F1 score: 0.4416431367807509\n",
      "\n",
      "-----Epoch 10/250-----\n",
      "Batch 150/485, loss: 0.5802235848704974 (0.492s)\n",
      "Batch 300/485, loss: 0.6568369186917941 (0.490s)\n",
      "Batch 450/485, loss: 0.5683001606663068 (0.490s)\n",
      "Batch 485/485, loss: 0.9324681375707898 (0.114s)\n",
      "F1 score: 0.4480255079996033\n",
      "\n",
      "-----Epoch 11/250-----\n",
      "Batch 150/485, loss: 0.5471715377767881 (0.492s)\n",
      "Batch 300/485, loss: 0.6010617387294769 (0.490s)\n",
      "Batch 450/485, loss: 0.6441325244307518 (0.490s)\n",
      "Batch 485/485, loss: 0.48538123411791667 (0.114s)\n",
      "F1 score: 0.454841938715851\n",
      "\n",
      "-----Epoch 12/250-----\n",
      "Batch 150/485, loss: 0.5335782259702683 (0.492s)\n",
      "Batch 300/485, loss: 0.6089461798469226 (0.490s)\n",
      "Batch 450/485, loss: 0.5650260078907013 (0.490s)\n",
      "Batch 485/485, loss: 0.5684606437172208 (0.114s)\n",
      "F1 score: 0.4652409928791039\n",
      "\n",
      "-----Epoch 13/250-----\n",
      "Batch 150/485, loss: 0.5885915686686833 (0.491s)\n",
      "Batch 300/485, loss: 0.58400072991848 (0.490s)\n",
      "Batch 450/485, loss: 0.5010058055321376 (0.489s)\n",
      "Batch 485/485, loss: 0.4056465583188193 (0.114s)\n",
      "F1 score: 0.4668019631748448\n",
      "\n",
      "-----Epoch 14/250-----\n",
      "Batch 150/485, loss: 0.5585709190368653 (0.492s)\n",
      "Batch 300/485, loss: 0.5311938732862472 (0.490s)\n",
      "Batch 450/485, loss: 0.49434386352698007 (0.489s)\n",
      "Batch 485/485, loss: 0.5940167808106968 (0.115s)\n",
      "F1 score: 0.47444706410197257\n",
      "\n",
      "-----Epoch 15/250-----\n",
      "Batch 150/485, loss: 0.4572975693643093 (0.492s)\n",
      "Batch 300/485, loss: 0.49957563360532126 (0.490s)\n",
      "Batch 450/485, loss: 0.5910027110079924 (0.489s)\n",
      "Batch 485/485, loss: 0.4682683506182262 (0.115s)\n",
      "F1 score: 0.4799090115647628\n",
      "\n",
      "-----Epoch 16/250-----\n",
      "Batch 150/485, loss: 0.4275925155977408 (0.492s)\n",
      "Batch 300/485, loss: 0.6326286914944649 (0.491s)\n",
      "Batch 450/485, loss: 0.5373490429421266 (0.489s)\n",
      "Batch 485/485, loss: 0.29638803218092236 (0.115s)\n",
      "F1 score: 0.4840650763998001\n",
      "\n",
      "-----Epoch 17/250-----\n",
      "Batch 150/485, loss: 0.5133831404149533 (0.491s)\n",
      "Batch 300/485, loss: 0.49523218964536986 (0.490s)\n",
      "Batch 450/485, loss: 0.4485156504313151 (0.490s)\n",
      "Batch 485/485, loss: 0.40371597324098857 (0.114s)\n",
      "F1 score: 0.4944975877358044\n",
      "\n",
      "-----Epoch 18/250-----\n",
      "Batch 150/485, loss: 0.428826614767313 (0.492s)\n",
      "Batch 300/485, loss: 0.5161347237726053 (0.491s)\n",
      "Batch 450/485, loss: 0.42915231655041375 (0.490s)\n",
      "Batch 485/485, loss: 0.52069859121527 (0.115s)\n",
      "F1 score: 0.49992824225076804\n",
      "\n",
      "-----Epoch 19/250-----\n",
      "Batch 150/485, loss: 0.453029375821352 (0.492s)\n",
      "Batch 300/485, loss: 0.4883536350230376 (0.490s)\n",
      "Batch 450/485, loss: 0.40742172141869865 (0.490s)\n",
      "Batch 485/485, loss: 0.37986933929579597 (0.114s)\n",
      "F1 score: 0.5087763962400776\n",
      "\n",
      "-----Epoch 20/250-----\n",
      "Batch 150/485, loss: 0.29455906569957735 (0.492s)\n",
      "Batch 300/485, loss: 0.5261316298941772 (0.490s)\n",
      "Batch 450/485, loss: 0.5025746572514375 (0.490s)\n",
      "Batch 485/485, loss: 0.27859286602054323 (0.115s)\n",
      "F1 score: 0.5164743032768577\n",
      "\n",
      "-----Epoch 21/250-----\n",
      "Batch 150/485, loss: 0.3980902113517125 (0.492s)\n",
      "Batch 300/485, loss: 0.454524167428414 (0.490s)\n",
      "Batch 450/485, loss: 0.38829882770776747 (0.490s)\n",
      "Batch 485/485, loss: 0.4553955444267818 (0.114s)\n",
      "F1 score: 0.5253693012524311\n",
      "\n",
      "-----Epoch 22/250-----\n",
      "Batch 150/485, loss: 0.4447383783757687 (0.491s)\n",
      "Batch 300/485, loss: 0.4098527730504672 (0.490s)\n",
      "Batch 450/485, loss: 0.4299151433010896 (0.489s)\n",
      "Batch 485/485, loss: 0.4917642689176968 (0.114s)\n",
      "F1 score: 0.5183733773217565\n",
      "\n",
      "-----Epoch 23/250-----\n",
      "Batch 150/485, loss: 0.39829447607199353 (0.491s)\n",
      "Batch 300/485, loss: 0.4097070150077343 (0.490s)\n",
      "Batch 450/485, loss: 0.43432528505722684 (0.492s)\n",
      "Batch 485/485, loss: 0.2676921574132783 (0.115s)\n",
      "F1 score: 0.5345598527396217\n",
      "\n",
      "-----Epoch 24/250-----\n",
      "Batch 150/485, loss: 0.45040669580300646 (0.494s)\n",
      "Batch 300/485, loss: 0.2920947320759296 (0.492s)\n",
      "Batch 450/485, loss: 0.4093866023917993 (0.491s)\n",
      "Batch 485/485, loss: 0.460875218468053 (0.115s)\n",
      "F1 score: 0.5471918628289174\n",
      "\n",
      "-----Epoch 25/250-----\n",
      "Batch 150/485, loss: 0.3968270915746689 (0.493s)\n",
      "Batch 300/485, loss: 0.3707064763704936 (0.491s)\n",
      "Batch 450/485, loss: 0.4256144406894843 (0.490s)\n",
      "Batch 485/485, loss: 0.21763020988021578 (0.115s)\n",
      "Saved to /home/hice1/khom9/CSE 8803 BMI Final Project/models/cnn_model-1.pth\n",
      "F1 score: 0.5523480865378955\n",
      "\n",
      "-----Epoch 26/250-----\n",
      "Batch 150/485, loss: 0.34104961678385737 (0.494s)\n",
      "Batch 300/485, loss: 0.3901742938657602 (0.491s)\n",
      "Batch 450/485, loss: 0.43460067038734757 (0.491s)\n",
      "Batch 485/485, loss: 0.4346185654401779 (0.115s)\n",
      "F1 score: 0.5519738163793259\n",
      "\n",
      "-----Epoch 27/250-----\n",
      "Batch 150/485, loss: 0.47269698590040204 (0.493s)\n",
      "Batch 300/485, loss: 0.305809512535731 (0.491s)\n",
      "Batch 450/485, loss: 0.42925444170832633 (0.490s)\n",
      "Batch 485/485, loss: 0.3132806196808815 (0.115s)\n",
      "F1 score: 0.5475441627732603\n",
      "\n",
      "-----Epoch 28/250-----\n",
      "Batch 150/485, loss: 0.3777944853901863 (0.493s)\n",
      "Batch 300/485, loss: 0.3117155014971892 (0.491s)\n",
      "Batch 450/485, loss: 0.4206843111415704 (0.490s)\n",
      "Batch 485/485, loss: 0.443460596033505 (0.115s)\n",
      "F1 score: 0.5694541886651582\n",
      "\n",
      "-----Epoch 29/250-----\n",
      "Batch 150/485, loss: 0.36466983025272687 (0.493s)\n",
      "Batch 300/485, loss: 0.3326428936918577 (0.491s)\n",
      "Batch 450/485, loss: 0.32445354099075 (0.490s)\n",
      "Batch 485/485, loss: 0.6085191366927964 (0.115s)\n",
      "F1 score: 0.580985127283809\n",
      "\n",
      "-----Epoch 30/250-----\n",
      "Batch 150/485, loss: 0.3989625438302755 (0.493s)\n",
      "Batch 300/485, loss: 0.32217394188046455 (0.491s)\n",
      "Batch 450/485, loss: 0.37385768607258796 (0.490s)\n",
      "Batch 485/485, loss: 0.32851604925734657 (0.115s)\n",
      "F1 score: 0.5805812809698679\n",
      "\n",
      "-----Epoch 31/250-----\n",
      "Batch 150/485, loss: 0.2908813586334387 (0.494s)\n",
      "Batch 300/485, loss: 0.4581542894740899 (0.491s)\n",
      "Batch 450/485, loss: 0.3326224495222171 (0.490s)\n",
      "Batch 485/485, loss: 0.2876821377447673 (0.115s)\n",
      "F1 score: 0.589129155526419\n",
      "\n",
      "-----Epoch 32/250-----\n",
      "Batch 150/485, loss: 0.2900440641740958 (0.493s)\n",
      "Batch 300/485, loss: 0.4898322073370218 (0.491s)\n",
      "Batch 450/485, loss: 0.3384686316549778 (0.490s)\n",
      "Batch 485/485, loss: 0.32368509556565966 (0.115s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.5795351748219533\n",
      "\n",
      "-----Epoch 33/250-----\n",
      "Batch 150/485, loss: 0.4150058332830667 (0.494s)\n",
      "Batch 300/485, loss: 0.33097900440295536 (0.491s)\n",
      "Batch 450/485, loss: 0.32575667060911656 (0.490s)\n",
      "Batch 485/485, loss: 0.38343204600470404 (0.114s)\n",
      "F1 score: 0.5897770519181216\n",
      "\n",
      "-----Epoch 34/250-----\n",
      "Batch 150/485, loss: 0.3095662558078766 (0.493s)\n",
      "Batch 300/485, loss: 0.35924288637936114 (0.491s)\n",
      "Batch 450/485, loss: 0.3295237411806981 (0.490s)\n",
      "Batch 485/485, loss: 0.3703146516212395 (0.114s)\n",
      "F1 score: 0.611008721540474\n",
      "\n",
      "-----Epoch 35/250-----\n",
      "Batch 150/485, loss: 0.31012739256024363 (0.492s)\n",
      "Batch 300/485, loss: 0.30384995403389137 (0.490s)\n",
      "Batch 450/485, loss: 0.31168015653888387 (0.490s)\n",
      "Batch 485/485, loss: 0.5687453356172357 (0.115s)\n",
      "F1 score: 0.6245788982668756\n",
      "\n",
      "-----Epoch 36/250-----\n",
      "Batch 150/485, loss: 0.36755525164306163 (0.493s)\n",
      "Batch 300/485, loss: 0.23648883566260337 (0.490s)\n",
      "Batch 450/485, loss: 0.35659697058300177 (0.491s)\n",
      "Batch 485/485, loss: 0.38842276600854736 (0.115s)\n",
      "F1 score: 0.6275521633661089\n",
      "\n",
      "-----Epoch 37/250-----\n",
      "Batch 150/485, loss: 0.38541441900034745 (0.493s)\n",
      "Batch 300/485, loss: 0.24524777901669342 (0.491s)\n",
      "Batch 450/485, loss: 0.29037578992545604 (0.491s)\n",
      "Batch 485/485, loss: 0.5163689535643373 (0.115s)\n",
      "F1 score: 0.631356453410448\n",
      "\n",
      "-----Epoch 38/250-----\n",
      "Batch 150/485, loss: 0.2901338122288386 (0.493s)\n",
      "Batch 300/485, loss: 0.39438396252691743 (0.491s)\n",
      "Batch 450/485, loss: 0.32090177103877066 (0.490s)\n",
      "Batch 485/485, loss: 0.2276502800839288 (0.115s)\n",
      "F1 score: 0.6289545971906882\n",
      "\n",
      "-----Epoch 39/250-----\n",
      "Batch 150/485, loss: 0.19889185746510823 (0.492s)\n",
      "Batch 300/485, loss: 0.389533583521843 (0.492s)\n",
      "Batch 450/485, loss: 0.3767770732442538 (0.491s)\n",
      "Batch 485/485, loss: 0.3660225474408695 (0.115s)\n",
      "F1 score: 0.6360682049273638\n",
      "\n",
      "-----Epoch 40/250-----\n",
      "Batch 150/485, loss: 0.3399134588241577 (0.493s)\n",
      "Batch 300/485, loss: 0.33548140831291673 (0.491s)\n",
      "Batch 450/485, loss: 0.2706425862014294 (0.491s)\n",
      "Batch 485/485, loss: 0.3802656990076814 (0.115s)\n",
      "F1 score: 0.6472573107123651\n",
      "\n",
      "-----Epoch 41/250-----\n",
      "Batch 150/485, loss: 0.29963341074685257 (0.493s)\n",
      "Batch 300/485, loss: 0.3498456217845281 (0.491s)\n",
      "Batch 450/485, loss: 0.35808683986465134 (0.490s)\n",
      "Batch 485/485, loss: 0.1942540881889207 (0.115s)\n",
      "F1 score: 0.6386050553736962\n",
      "\n",
      "-----Epoch 42/250-----\n",
      "Batch 150/485, loss: 0.28928642041981223 (0.493s)\n",
      "Batch 300/485, loss: 0.2818904630343119 (0.491s)\n",
      "Batch 450/485, loss: 0.3188898002604644 (0.491s)\n",
      "Batch 485/485, loss: 0.5595228402742318 (0.115s)\n",
      "F1 score: 0.6482417269653705\n",
      "\n",
      "-----Epoch 43/250-----\n",
      "Batch 150/485, loss: 0.36463889315724374 (0.492s)\n",
      "Batch 300/485, loss: 0.3093618912498156 (0.491s)\n",
      "Batch 450/485, loss: 0.27931522369384765 (0.490s)\n",
      "Batch 485/485, loss: 0.26575027502008847 (0.115s)\n",
      "F1 score: 0.658340971467539\n",
      "\n",
      "-----Epoch 44/250-----\n",
      "Batch 150/485, loss: 0.27761263844867545 (0.492s)\n",
      "Batch 300/485, loss: 0.21292239936689536 (0.491s)\n",
      "Batch 450/485, loss: 0.4400702359775702 (0.490s)\n",
      "Batch 485/485, loss: 0.3907217219471931 (0.115s)\n",
      "F1 score: 0.6587212782338154\n",
      "\n",
      "-----Epoch 45/250-----\n",
      "Batch 150/485, loss: 0.32851267139116924 (0.493s)\n",
      "Batch 300/485, loss: 0.356647282615304 (0.491s)\n",
      "Batch 450/485, loss: 0.26886457997063795 (0.491s)\n",
      "Batch 485/485, loss: 0.2799926179860319 (0.115s)\n",
      "F1 score: 0.6547316731227523\n",
      "\n",
      "-----Epoch 46/250-----\n",
      "Batch 150/485, loss: 0.21657307510574658 (0.494s)\n",
      "Batch 300/485, loss: 0.3571623474359512 (0.492s)\n",
      "Batch 450/485, loss: 0.4117342404772838 (0.493s)\n",
      "Batch 485/485, loss: 0.2680492138223989 (0.116s)\n",
      "F1 score: 0.6584535849675753\n",
      "\n",
      "-----Epoch 47/250-----\n",
      "Batch 150/485, loss: 0.32342107343177 (0.495s)\n",
      "Batch 300/485, loss: 0.3668903813759486 (0.493s)\n",
      "Batch 450/485, loss: 0.243584726502498 (0.493s)\n",
      "Batch 485/485, loss: 0.4053774927343641 (0.115s)\n",
      "F1 score: 0.6525530547230226\n",
      "\n",
      "-----Epoch 48/250-----\n",
      "Batch 150/485, loss: 0.24718961680928866 (0.496s)\n",
      "Batch 300/485, loss: 0.3217433956762155 (0.494s)\n",
      "Batch 450/485, loss: 0.35343240829805533 (0.492s)\n",
      "Batch 485/485, loss: 0.26437204756907057 (0.115s)\n",
      "F1 score: 0.6779226859661835\n",
      "\n",
      "-----Epoch 49/250-----\n",
      "Batch 150/485, loss: 0.317253757417202 (0.496s)\n",
      "Batch 300/485, loss: 0.23987214602530002 (0.493s)\n",
      "Batch 450/485, loss: 0.32731571483115357 (0.493s)\n",
      "Batch 485/485, loss: 0.4048823264028345 (0.115s)\n",
      "F1 score: 0.683526735562161\n",
      "\n",
      "-----Epoch 50/250-----\n",
      "Batch 150/485, loss: 0.28001700242360433 (0.495s)\n",
      "Batch 300/485, loss: 0.3602989687770605 (0.493s)\n",
      "Batch 450/485, loss: 0.28835886421302953 (0.493s)\n",
      "Batch 485/485, loss: 0.30620552812303814 (0.115s)\n",
      "Saved to /home/hice1/khom9/CSE 8803 BMI Final Project/models/cnn_model-1.pth\n",
      "F1 score: 0.6725609581119963\n",
      "\n",
      "-----Epoch 51/250-----\n",
      "Batch 150/485, loss: 0.2825941619773706 (0.496s)\n",
      "Batch 300/485, loss: 0.3169664544115464 (0.494s)\n",
      "Batch 450/485, loss: 0.27116079171498614 (0.493s)\n",
      "Batch 485/485, loss: 0.4423274055123329 (0.115s)\n",
      "F1 score: 0.6927340818072777\n",
      "\n",
      "-----Epoch 52/250-----\n",
      "Batch 150/485, loss: 0.266639662583669 (0.494s)\n",
      "Batch 300/485, loss: 0.29634641023973624 (0.493s)\n",
      "Batch 450/485, loss: 0.3073476133743922 (0.492s)\n",
      "Batch 485/485, loss: 0.33161360025405884 (0.115s)\n",
      "F1 score: 0.713264714546939\n",
      "\n",
      "-----Epoch 53/250-----\n",
      "Batch 150/485, loss: 0.3039593676105142 (0.495s)\n",
      "Batch 300/485, loss: 0.24879574423034986 (0.492s)\n",
      "Batch 450/485, loss: 0.2954044754058123 (0.491s)\n",
      "Batch 485/485, loss: 0.3929762799292803 (0.115s)\n",
      "F1 score: 0.7214766907650785\n",
      "\n",
      "-----Epoch 54/250-----\n",
      "Batch 150/485, loss: 0.25634796895086764 (0.493s)\n",
      "Batch 300/485, loss: 0.3349841708689928 (0.493s)\n",
      "Batch 450/485, loss: 0.26815168641507625 (0.492s)\n",
      "Batch 485/485, loss: 0.31902058092611174 (0.115s)\n",
      "F1 score: 0.728697170514708\n",
      "\n",
      "-----Epoch 55/250-----\n",
      "Batch 150/485, loss: 0.2651830618456006 (0.493s)\n",
      "Batch 300/485, loss: 0.3140941381206115 (0.493s)\n",
      "Batch 450/485, loss: 0.29782625174770755 (0.491s)\n",
      "Batch 485/485, loss: 0.2783954783209733 (0.115s)\n",
      "F1 score: 0.723981816416869\n",
      "\n",
      "-----Epoch 56/250-----\n",
      "Batch 150/485, loss: 0.3407339443763097 (0.494s)\n",
      "Batch 300/485, loss: 0.22133343933771055 (0.492s)\n",
      "Batch 450/485, loss: 0.3477348915860057 (0.490s)\n",
      "Batch 485/485, loss: 0.08856814045991217 (0.114s)\n",
      "F1 score: 0.7344380631656223\n",
      "\n",
      "-----Epoch 57/250-----\n",
      "Batch 150/485, loss: 0.31497616271177925 (0.494s)\n",
      "Batch 300/485, loss: 0.334361292347312 (0.491s)\n",
      "Batch 450/485, loss: 0.2529545677577456 (0.491s)\n",
      "Batch 485/485, loss: 0.10471430484737669 (0.115s)\n",
      "F1 score: 0.7391140769921082\n",
      "\n",
      "-----Epoch 58/250-----\n",
      "Batch 150/485, loss: 0.2521375721941392 (0.494s)\n",
      "Batch 300/485, loss: 0.29420883012314636 (0.492s)\n",
      "Batch 450/485, loss: 0.3489327125872175 (0.490s)\n",
      "Batch 485/485, loss: 0.1647592504109655 (0.114s)\n",
      "F1 score: 0.737295577422248\n",
      "\n",
      "-----Epoch 59/250-----\n",
      "Batch 150/485, loss: 0.31357407197356224 (0.495s)\n",
      "Batch 300/485, loss: 0.3271259537835916 (0.491s)\n",
      "Batch 450/485, loss: 0.262581165942053 (0.491s)\n",
      "Batch 485/485, loss: 0.21436610812587398 (0.115s)\n",
      "F1 score: 0.718687530386665\n",
      "\n",
      "-----Epoch 60/250-----\n",
      "Batch 150/485, loss: 0.2674423940976461 (0.493s)\n",
      "Batch 300/485, loss: 0.29375450080881516 (0.491s)\n",
      "Batch 450/485, loss: 0.3156284917270144 (0.491s)\n",
      "Batch 485/485, loss: 0.18865679635533264 (0.115s)\n",
      "F1 score: 0.7473705269831452\n",
      "\n",
      "-----Epoch 61/250-----\n",
      "Batch 150/485, loss: 0.2276972945034504 (0.494s)\n",
      "Batch 300/485, loss: 0.3200153994187713 (0.492s)\n",
      "Batch 450/485, loss: 0.3161136612916986 (0.491s)\n",
      "Batch 485/485, loss: 0.22056490975831236 (0.115s)\n",
      "F1 score: 0.7523417458952877\n",
      "\n",
      "-----Epoch 62/250-----\n",
      "Batch 150/485, loss: 0.17790016087392965 (0.493s)\n",
      "Batch 300/485, loss: 0.3719689718882243 (0.491s)\n",
      "Batch 450/485, loss: 0.26598279893398286 (0.490s)\n",
      "Batch 485/485, loss: 0.42473448419145177 (0.115s)\n",
      "F1 score: 0.7553802150410631\n",
      "\n",
      "-----Epoch 63/250-----\n",
      "Batch 150/485, loss: 0.3155822572360436 (0.493s)\n",
      "Batch 300/485, loss: 0.31414303958415984 (0.492s)\n",
      "Batch 450/485, loss: 0.22774608340114355 (0.491s)\n",
      "Batch 485/485, loss: 0.29617679720478396 (0.115s)\n",
      "F1 score: 0.7471199381895607\n",
      "\n",
      "-----Epoch 64/250-----\n",
      "Batch 150/485, loss: 0.2814586015666525 (0.493s)\n",
      "Batch 300/485, loss: 0.2611996093764901 (0.491s)\n",
      "Batch 450/485, loss: 0.26995473631968103 (0.491s)\n",
      "Batch 485/485, loss: 0.4654977519597326 (0.115s)\n",
      "F1 score: 0.7556259845686093\n",
      "\n",
      "-----Epoch 65/250-----\n",
      "Batch 150/485, loss: 0.30533495972553887 (0.494s)\n",
      "Batch 300/485, loss: 0.235937981903553 (0.491s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 450/485, loss: 0.3073887616644303 (0.491s)\n",
      "Batch 485/485, loss: 0.22723393477499484 (0.115s)\n",
      "F1 score: 0.7683184917451736\n",
      "\n",
      "-----Epoch 66/250-----\n",
      "Batch 150/485, loss: 0.3831789926439524 (0.494s)\n",
      "Batch 300/485, loss: 0.18703221843888362 (0.492s)\n",
      "Batch 450/485, loss: 0.2835490345209837 (0.492s)\n",
      "Batch 485/485, loss: 0.18947035553199904 (0.115s)\n",
      "F1 score: 0.7743136516336099\n",
      "\n",
      "-----Epoch 67/250-----\n",
      "Batch 150/485, loss: 0.2747685305898388 (0.494s)\n",
      "Batch 300/485, loss: 0.22395578128596147 (0.491s)\n",
      "Batch 450/485, loss: 0.3393317119901379 (0.491s)\n",
      "Batch 485/485, loss: 0.29482312122625964 (0.115s)\n",
      "F1 score: 0.7673962488685404\n",
      "\n",
      "-----Epoch 68/250-----\n",
      "Batch 150/485, loss: 0.3358397975564003 (0.494s)\n",
      "Batch 300/485, loss: 0.2390079396838943 (0.492s)\n",
      "Batch 450/485, loss: 0.2671579742555817 (0.492s)\n",
      "Batch 485/485, loss: 0.21001040350113595 (0.116s)\n",
      "F1 score: 0.7827009139104966\n",
      "\n",
      "-----Epoch 69/250-----\n",
      "Batch 150/485, loss: 0.23379149213433265 (0.493s)\n",
      "Batch 300/485, loss: 0.27271334127833446 (0.492s)\n",
      "Batch 450/485, loss: 0.3214246211325129 (0.491s)\n",
      "Batch 485/485, loss: 0.3376467097018446 (0.115s)\n",
      "F1 score: 0.7758788574607135\n",
      "\n",
      "-----Epoch 70/250-----\n",
      "Batch 150/485, loss: 0.2409879683579008 (0.493s)\n",
      "Batch 300/485, loss: 0.29106936141848566 (0.492s)\n",
      "Batch 450/485, loss: 0.3334558713187774 (0.492s)\n",
      "Batch 485/485, loss: 0.09263842249555247 (0.115s)\n",
      "F1 score: 0.7904080244681354\n",
      "\n",
      "-----Epoch 71/250-----\n",
      "Batch 150/485, loss: 0.25931208197027444 (0.494s)\n",
      "Batch 300/485, loss: 0.253786254140238 (0.492s)\n",
      "Batch 450/485, loss: 0.3143387487779061 (0.491s)\n",
      "Batch 485/485, loss: 0.2978252400244985 (0.115s)\n",
      "F1 score: 0.7814608589987319\n",
      "\n",
      "-----Epoch 72/250-----\n",
      "Batch 150/485, loss: 0.21059292297810317 (0.494s)\n",
      "Batch 300/485, loss: 0.3847370951250195 (0.492s)\n",
      "Batch 450/485, loss: 0.21653687622398138 (0.492s)\n",
      "Batch 485/485, loss: 0.29578098697321753 (0.115s)\n",
      "F1 score: 0.7977715697824723\n",
      "\n",
      "-----Epoch 73/250-----\n",
      "Batch 150/485, loss: 0.24141322133441767 (0.495s)\n",
      "Batch 300/485, loss: 0.2324689352636536 (0.492s)\n",
      "Batch 450/485, loss: 0.3743912569681803 (0.492s)\n",
      "Batch 485/485, loss: 0.28956062900168555 (0.115s)\n",
      "F1 score: 0.7852771698250088\n",
      "\n",
      "-----Epoch 74/250-----\n",
      "Batch 150/485, loss: 0.2660439005245765 (0.494s)\n",
      "Batch 300/485, loss: 0.3444937257096171 (0.492s)\n",
      "Batch 450/485, loss: 0.26301783123364053 (0.492s)\n",
      "Batch 485/485, loss: 0.061949763926012175 (0.115s)\n",
      "F1 score: 0.786006606488503\n",
      "\n",
      "-----Epoch 75/250-----\n",
      "Batch 150/485, loss: 0.17177055011192957 (0.494s)\n",
      "Batch 300/485, loss: 0.25621445551514627 (0.491s)\n",
      "Batch 450/485, loss: 0.3627504551038146 (0.491s)\n",
      "Batch 485/485, loss: 0.36348008806152005 (0.115s)\n",
      "Saved to /home/hice1/khom9/CSE 8803 BMI Final Project/models/cnn_model-1.pth\n",
      "F1 score: 0.8053620937619985\n",
      "\n",
      "-----Epoch 76/250-----\n",
      "Batch 150/485, loss: 0.2588353866835435 (0.495s)\n",
      "Batch 300/485, loss: 0.24154493084798256 (0.492s)\n",
      "Batch 450/485, loss: 0.36458662966887156 (0.491s)\n",
      "Batch 485/485, loss: 0.09027003209505763 (0.115s)\n",
      "F1 score: 0.7922006013310253\n",
      "\n",
      "-----Epoch 77/250-----\n",
      "Batch 150/485, loss: 0.2937526205802957 (0.494s)\n",
      "Batch 300/485, loss: 0.29285340547561645 (0.492s)\n",
      "Batch 450/485, loss: 0.22899642566839853 (0.492s)\n",
      "Batch 485/485, loss: 0.2507904263479369 (0.115s)\n",
      "F1 score: 0.8061612710565281\n",
      "\n",
      "-----Epoch 78/250-----\n",
      "Batch 150/485, loss: 0.289204807455341 (0.494s)\n",
      "Batch 300/485, loss: 0.19209824410577614 (0.491s)\n",
      "Batch 450/485, loss: 0.3322541107982397 (0.490s)\n",
      "Batch 485/485, loss: 0.35272146144083566 (0.115s)\n",
      "F1 score: 0.7911703540333838\n",
      "\n",
      "-----Epoch 79/250-----\n",
      "Batch 150/485, loss: 0.19613399708022675 (0.493s)\n",
      "Batch 300/485, loss: 0.3111377169812719 (0.491s)\n",
      "Batch 450/485, loss: 0.28111141089349984 (0.491s)\n",
      "Batch 485/485, loss: 0.4136186036680426 (0.115s)\n",
      "F1 score: 0.7942365477589693\n",
      "\n",
      "-----Epoch 80/250-----\n",
      "Batch 150/485, loss: 0.17382458593696357 (0.494s)\n",
      "Batch 300/485, loss: 0.30893663292129836 (0.492s)\n",
      "Batch 450/485, loss: 0.3171513738979896 (0.491s)\n",
      "Batch 485/485, loss: 0.4185152120888233 (0.114s)\n",
      "F1 score: 0.805215658173302\n",
      "\n",
      "-----Epoch 81/250-----\n",
      "Batch 150/485, loss: 0.247763696834445 (0.494s)\n",
      "Batch 300/485, loss: 0.25239497020840646 (0.492s)\n",
      "Batch 450/485, loss: 0.3352164342502753 (0.491s)\n",
      "Batch 485/485, loss: 0.19830782482666628 (0.115s)\n",
      "F1 score: 0.7960333694056555\n",
      "\n",
      "-----Epoch 82/250-----\n",
      "Batch 150/485, loss: 0.27199730431040126 (0.495s)\n",
      "Batch 300/485, loss: 0.2190915673598647 (0.492s)\n",
      "Batch 450/485, loss: 0.32200104527175427 (0.492s)\n",
      "Batch 485/485, loss: 0.2653559376086507 (0.115s)\n",
      "F1 score: 0.8114242525764926\n",
      "\n",
      "-----Epoch 83/250-----\n",
      "Batch 150/485, loss: 0.3519407955557108 (0.494s)\n",
      "Batch 300/485, loss: 0.2179947472239534 (0.490s)\n",
      "Batch 450/485, loss: 0.2722216453651587 (0.491s)\n",
      "Batch 485/485, loss: 0.15578563032405718 (0.115s)\n",
      "F1 score: 0.802522502478986\n",
      "\n",
      "-----Epoch 84/250-----\n",
      "Batch 150/485, loss: 0.26053666602199277 (0.494s)\n",
      "Batch 300/485, loss: 0.37825623581806816 (0.492s)\n",
      "Batch 450/485, loss: 0.195670998506248 (0.491s)\n",
      "Batch 485/485, loss: 0.13299997305231434 (0.115s)\n",
      "F1 score: 0.8187743364380863\n",
      "\n",
      "-----Epoch 85/250-----\n",
      "Batch 150/485, loss: 0.27000163480639455 (0.495s)\n",
      "Batch 300/485, loss: 0.27406594982991617 (0.493s)\n",
      "Batch 450/485, loss: 0.24331265478084485 (0.492s)\n",
      "Batch 485/485, loss: 0.31569111049175264 (0.115s)\n",
      "F1 score: 0.8244676068355481\n",
      "\n",
      "-----Epoch 86/250-----\n",
      "Batch 150/485, loss: 0.26481819612905383 (0.494s)\n",
      "Batch 300/485, loss: 0.3145495593858262 (0.493s)\n",
      "Batch 450/485, loss: 0.20841215488811335 (0.491s)\n",
      "Batch 485/485, loss: 0.3216964457184076 (0.115s)\n",
      "F1 score: 0.8244734743291201\n",
      "\n",
      "-----Epoch 87/250-----\n",
      "Batch 150/485, loss: 0.3512902458446721 (0.496s)\n",
      "Batch 300/485, loss: 0.23518489961822828 (0.496s)\n",
      "Batch 450/485, loss: 0.23376871803154547 (0.491s)\n",
      "Batch 485/485, loss: 0.15697262159415654 (0.115s)\n",
      "F1 score: 0.8315534527971622\n",
      "\n",
      "-----Epoch 88/250-----\n",
      "Batch 150/485, loss: 0.19776607195536294 (0.493s)\n",
      "Batch 300/485, loss: 0.22782312611738842 (0.492s)\n",
      "Batch 450/485, loss: 0.30217097208524746 (0.491s)\n",
      "Batch 485/485, loss: 0.5375232800309148 (0.115s)\n",
      "F1 score: 0.8390331262095503\n",
      "\n",
      "-----Epoch 89/250-----\n",
      "Batch 150/485, loss: 0.26338012975951036 (0.492s)\n",
      "Batch 300/485, loss: 0.273763214237988 (0.491s)\n",
      "Batch 450/485, loss: 0.2521499693890413 (0.489s)\n",
      "Batch 485/485, loss: 0.27744731727455346 (0.114s)\n",
      "F1 score: 0.8377065154198317\n",
      "\n",
      "-----Epoch 90/250-----\n",
      "Batch 150/485, loss: 0.23673407330488166 (0.491s)\n",
      "Batch 300/485, loss: 0.31812745016689103 (0.489s)\n",
      "Batch 450/485, loss: 0.21560729417949914 (0.489s)\n",
      "Batch 485/485, loss: 0.370687342860869 (0.114s)\n",
      "F1 score: 0.8387959367362566\n",
      "\n",
      "-----Epoch 91/250-----\n",
      "Batch 150/485, loss: 0.21707725253577034 (0.492s)\n",
      "Batch 300/485, loss: 0.2501315165311098 (0.490s)\n",
      "Batch 450/485, loss: 0.3440737000728647 (0.489s)\n",
      "Batch 485/485, loss: 0.3325519951858691 (0.114s)\n",
      "F1 score: 0.8193306341395196\n",
      "\n",
      "-----Epoch 92/250-----\n",
      "Batch 150/485, loss: 0.2710557898754875 (0.493s)\n",
      "Batch 300/485, loss: 0.36238693342233697 (0.490s)\n",
      "Batch 450/485, loss: 0.19987395452956358 (0.488s)\n",
      "Batch 485/485, loss: 0.12064038190458502 (0.114s)\n",
      "F1 score: 0.8293675253498164\n",
      "\n",
      "-----Epoch 93/250-----\n",
      "Batch 150/485, loss: 0.26302573005358376 (0.492s)\n",
      "Batch 300/485, loss: 0.27220189516743026 (0.490s)\n",
      "Batch 450/485, loss: 0.2525138408690691 (0.489s)\n",
      "Batch 485/485, loss: 0.30497451701334544 (0.114s)\n",
      "F1 score: 0.8406523842976414\n",
      "\n",
      "-----Epoch 94/250-----\n",
      "Batch 150/485, loss: 0.1993885478625695 (0.491s)\n",
      "Batch 300/485, loss: 0.27303276749327776 (0.489s)\n",
      "Batch 450/485, loss: 0.29898286911969385 (0.489s)\n",
      "Batch 485/485, loss: 0.34790185556880066 (0.115s)\n",
      "F1 score: 0.846686427510402\n",
      "\n",
      "-----Epoch 95/250-----\n",
      "Batch 150/485, loss: 0.24192328151936332 (0.492s)\n",
      "Batch 300/485, loss: 0.27298888214553396 (0.490s)\n",
      "Batch 450/485, loss: 0.20667280675222477 (0.489s)\n",
      "Batch 485/485, loss: 0.550289905497006 (0.114s)\n",
      "F1 score: 0.8480987938424869\n",
      "\n",
      "-----Epoch 96/250-----\n",
      "Batch 150/485, loss: 0.245477432521681 (0.491s)\n",
      "Batch 300/485, loss: 0.3601861467212439 (0.490s)\n",
      "Batch 450/485, loss: 0.21275168906276426 (0.488s)\n",
      "Batch 485/485, loss: 0.1188694021797606 (0.114s)\n",
      "F1 score: 0.8545726742091085\n",
      "\n",
      "-----Epoch 97/250-----\n",
      "Batch 150/485, loss: 0.2725805855480333 (0.492s)\n",
      "Batch 300/485, loss: 0.27716912117476267 (0.489s)\n",
      "Batch 450/485, loss: 0.19487204444905123 (0.489s)\n",
      "Batch 485/485, loss: 0.511375953150647 (0.114s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.8574429296163414\n",
      "\n",
      "-----Epoch 98/250-----\n",
      "Batch 150/485, loss: 0.23259851648161808 (0.491s)\n",
      "Batch 300/485, loss: 0.34695525697122015 (0.490s)\n",
      "Batch 450/485, loss: 0.2263740887803336 (0.489s)\n",
      "Batch 485/485, loss: 0.23940836143280778 (0.114s)\n",
      "F1 score: 0.8415471260239781\n",
      "\n",
      "-----Epoch 99/250-----\n",
      "Batch 150/485, loss: 0.20237840236475071 (0.491s)\n",
      "Batch 300/485, loss: 0.2687944476430615 (0.490s)\n",
      "Batch 450/485, loss: 0.3495612770753602 (0.489s)\n",
      "Batch 485/485, loss: 0.15440324920096568 (0.114s)\n",
      "F1 score: 0.8478883266786374\n",
      "\n",
      "-----Epoch 100/250-----\n",
      "Batch 150/485, loss: 0.23011758509402475 (0.491s)\n",
      "Batch 300/485, loss: 0.2906177298973004 (0.491s)\n",
      "Batch 450/485, loss: 0.27181106564278407 (0.494s)\n",
      "Batch 485/485, loss: 0.24668631896908794 (0.115s)\n",
      "Saved to /home/hice1/khom9/CSE 8803 BMI Final Project/models/cnn_model-1.pth\n",
      "F1 score: 0.8500103873914608\n",
      "\n",
      "-----Epoch 101/250-----\n",
      "Batch 150/485, loss: 0.24145625491937 (0.496s)\n",
      "Batch 300/485, loss: 0.2091798944460849 (0.495s)\n",
      "Batch 450/485, loss: 0.3216635955870151 (0.493s)\n",
      "Batch 485/485, loss: 0.2890399130859545 (0.116s)\n",
      "F1 score: 0.8656250020134609\n",
      "\n",
      "-----Epoch 102/250-----\n",
      "Batch 150/485, loss: 0.2573646104397873 (0.496s)\n",
      "Batch 300/485, loss: 0.2872312504798174 (0.494s)\n",
      "Batch 450/485, loss: 0.20789248468975227 (0.494s)\n",
      "Batch 485/485, loss: 0.3553871836779373 (0.115s)\n",
      "F1 score: 0.8709694170206046\n",
      "\n",
      "-----Epoch 103/250-----\n",
      "Batch 150/485, loss: 0.17757478602230548 (0.496s)\n",
      "Batch 300/485, loss: 0.21315287637213867 (0.494s)\n",
      "Batch 450/485, loss: 0.37739417572195333 (0.494s)\n",
      "Batch 485/485, loss: 0.28200820868036575 (0.115s)\n",
      "F1 score: 0.8733745483197596\n",
      "\n",
      "-----Epoch 104/250-----\n",
      "Batch 150/485, loss: 0.29783519977703693 (0.496s)\n",
      "Batch 300/485, loss: 0.19112685831884543 (0.494s)\n",
      "Batch 450/485, loss: 0.22703325329969326 (0.493s)\n",
      "Batch 485/485, loss: 0.5109468575300915 (0.115s)\n",
      "F1 score: 0.8734903627288344\n",
      "\n",
      "-----Epoch 105/250-----\n",
      "Batch 150/485, loss: 0.21103383569046855 (0.491s)\n",
      "Batch 300/485, loss: 0.21870678775012492 (0.489s)\n",
      "Batch 450/485, loss: 0.30584688027699786 (0.489s)\n",
      "Batch 485/485, loss: 0.4177751336778913 (0.114s)\n",
      "F1 score: 0.8769102281036488\n",
      "\n",
      "-----Epoch 106/250-----\n",
      "Batch 150/485, loss: 0.28019534210984903 (0.492s)\n",
      "Batch 300/485, loss: 0.28484156633416813 (0.490s)\n",
      "Batch 450/485, loss: 0.19021746720497806 (0.490s)\n",
      "Batch 485/485, loss: 0.3367974323619689 (0.115s)\n",
      "F1 score: 0.8760398032948008\n",
      "\n",
      "-----Epoch 107/250-----\n",
      "Batch 150/485, loss: 0.18359668208907048 (0.491s)\n",
      "Batch 300/485, loss: 0.342787668860207 (0.490s)\n",
      "Batch 450/485, loss: 0.24411366377025842 (0.490s)\n",
      "Batch 485/485, loss: 0.2638254542169826 (0.115s)\n",
      "F1 score: 0.8793267315622457\n",
      "\n",
      "-----Epoch 108/250-----\n",
      "Batch 150/485, loss: 0.3110590762582918 (0.491s)\n",
      "Batch 300/485, loss: 0.23744316736857096 (0.489s)\n",
      "Batch 450/485, loss: 0.2578429264947772 (0.488s)\n",
      "Batch 485/485, loss: 0.13080485141170875 (0.114s)\n",
      "F1 score: 0.8735170136279807\n",
      "\n",
      "-----Epoch 109/250-----\n",
      "Batch 150/485, loss: 0.2164048949070275 (0.491s)\n",
      "Batch 300/485, loss: 0.27824067883814374 (0.490s)\n",
      "Batch 450/485, loss: 0.24886654493709406 (0.489s)\n",
      "Batch 485/485, loss: 0.3764184896701149 (0.114s)\n",
      "F1 score: 0.8812066389217604\n",
      "\n",
      "-----Epoch 110/250-----\n",
      "Batch 150/485, loss: 0.2364301625949641 (0.491s)\n",
      "Batch 300/485, loss: 0.298182657007128 (0.489s)\n",
      "Batch 450/485, loss: 0.21792592464014887 (0.489s)\n",
      "Batch 485/485, loss: 0.3652956325294716 (0.114s)\n",
      "F1 score: 0.8734402432141513\n",
      "\n",
      "-----Epoch 111/250-----\n",
      "Batch 150/485, loss: 0.24484720695142945 (0.491s)\n",
      "Batch 300/485, loss: 0.23689856220036745 (0.490s)\n",
      "Batch 450/485, loss: 0.33008129059026636 (0.489s)\n",
      "Batch 485/485, loss: 0.0748313150501677 (0.114s)\n",
      "F1 score: 0.8834879319663946\n",
      "\n",
      "-----Epoch 112/250-----\n",
      "Batch 150/485, loss: 0.24613638570532204 (0.492s)\n",
      "Batch 300/485, loss: 0.22812494181406995 (0.489s)\n",
      "Batch 450/485, loss: 0.25470758613198996 (0.488s)\n",
      "Batch 485/485, loss: 0.42134450785815714 (0.114s)\n",
      "F1 score: 0.8872111076344124\n",
      "\n",
      "-----Epoch 113/250-----\n",
      "Batch 150/485, loss: 0.3329235489666462 (0.491s)\n",
      "Batch 300/485, loss: 0.23538545239716768 (0.489s)\n",
      "Batch 450/485, loss: 0.2324066936969757 (0.489s)\n",
      "Batch 485/485, loss: 0.15867156091013124 (0.114s)\n",
      "F1 score: 0.8775004216615433\n",
      "\n",
      "-----Epoch 114/250-----\n",
      "Batch 150/485, loss: 0.28061100529506805 (0.492s)\n",
      "Batch 300/485, loss: 0.16895605271061262 (0.490s)\n",
      "Batch 450/485, loss: 0.29417369541401667 (0.489s)\n",
      "Batch 485/485, loss: 0.3738226992477264 (0.114s)\n",
      "F1 score: 0.8814642624520838\n",
      "\n",
      "-----Epoch 115/250-----\n",
      "Batch 150/485, loss: 0.1782760641972224 (0.491s)\n",
      "Batch 300/485, loss: 0.33590350884323317 (0.489s)\n",
      "Batch 450/485, loss: 0.2369242495422562 (0.489s)\n",
      "Batch 485/485, loss: 0.3416517331664051 (0.114s)\n",
      "F1 score: 0.8852793028117973\n",
      "\n",
      "-----Epoch 116/250-----\n",
      "Batch 150/485, loss: 0.274129032616814 (0.491s)\n",
      "Batch 300/485, loss: 0.2976998110301793 (0.490s)\n",
      "Batch 450/485, loss: 0.25268952052419386 (0.490s)\n",
      "Batch 485/485, loss: 0.20760527215898036 (0.114s)\n",
      "F1 score: 0.8563597962681769\n",
      "\n",
      "-----Epoch 117/250-----\n",
      "Batch 150/485, loss: 0.2279952180199325 (0.491s)\n",
      "Batch 300/485, loss: 0.2928218610646824 (0.490s)\n",
      "Batch 450/485, loss: 0.25269144769757984 (0.488s)\n",
      "Batch 485/485, loss: 0.2541734058409929 (0.114s)\n",
      "F1 score: 0.8783868665722396\n",
      "\n",
      "-----Epoch 118/250-----\n",
      "Batch 150/485, loss: 0.19904318918784458 (0.491s)\n",
      "Batch 300/485, loss: 0.3418689476015667 (0.490s)\n",
      "Batch 450/485, loss: 0.2419419741630554 (0.489s)\n",
      "Batch 485/485, loss: 0.19089151953480074 (0.114s)\n",
      "F1 score: 0.8854810141410796\n",
      "\n",
      "-----Epoch 119/250-----\n",
      "Batch 150/485, loss: 0.20085622100780406 (0.492s)\n",
      "Batch 300/485, loss: 0.29836149907360476 (0.492s)\n",
      "Batch 450/485, loss: 0.24992687355106075 (0.491s)\n",
      "Batch 485/485, loss: 0.32987545957522735 (0.115s)\n",
      "F1 score: 0.8895333377890101\n",
      "\n",
      "-----Epoch 120/250-----\n",
      "Batch 150/485, loss: 0.2845480627939105 (0.494s)\n",
      "Batch 300/485, loss: 0.17922811224746207 (0.494s)\n",
      "Batch 450/485, loss: 0.33259635109454394 (0.494s)\n",
      "Batch 485/485, loss: 0.13340726548007556 (0.115s)\n",
      "F1 score: 0.8881271882513974\n",
      "\n",
      "-----Epoch 121/250-----\n",
      "Batch 150/485, loss: 0.19463254248102507 (0.497s)\n",
      "Batch 300/485, loss: 0.29758174126346904 (0.495s)\n",
      "Batch 450/485, loss: 0.2609082176412145 (0.494s)\n",
      "Batch 485/485, loss: 0.3146295802667737 (0.116s)\n",
      "F1 score: 0.8900837566116929\n",
      "\n",
      "-----Epoch 122/250-----\n",
      "Batch 150/485, loss: 0.2836094531416893 (0.496s)\n",
      "Batch 300/485, loss: 0.31773219072880843 (0.495s)\n",
      "Batch 450/485, loss: 0.20652090745046736 (0.494s)\n",
      "Batch 485/485, loss: 0.07354289697749274 (0.116s)\n",
      "F1 score: 0.892856725077652\n",
      "\n",
      "-----Epoch 123/250-----\n",
      "Batch 150/485, loss: 0.20703298358246683 (0.496s)\n",
      "Batch 300/485, loss: 0.28270676443663734 (0.492s)\n",
      "Batch 450/485, loss: 0.315832323183616 (0.492s)\n",
      "Batch 485/485, loss: 0.07514015575870872 (0.115s)\n",
      "F1 score: 0.8951608424933156\n",
      "\n",
      "-----Epoch 124/250-----\n",
      "Batch 150/485, loss: 0.2400538429028044 (0.494s)\n",
      "Batch 300/485, loss: 0.322392845597739 (0.493s)\n",
      "Batch 450/485, loss: 0.2174247597840925 (0.493s)\n",
      "Batch 485/485, loss: 0.18450832452092852 (0.115s)\n",
      "F1 score: 0.8966283099595244\n",
      "\n",
      "-----Epoch 125/250-----\n",
      "Batch 150/485, loss: 0.2473361320234835 (0.495s)\n",
      "Batch 300/485, loss: 0.3006979959147672 (0.490s)\n",
      "Batch 450/485, loss: 0.22064495980739593 (0.488s)\n",
      "Batch 485/485, loss: 0.2272883138353271 (0.114s)\n",
      "Saved to /home/hice1/khom9/CSE 8803 BMI Final Project/models/cnn_model-1.pth\n",
      "F1 score: 0.8979552493856984\n",
      "\n",
      "-----Epoch 126/250-----\n",
      "Batch 150/485, loss: 0.26081724649916094 (0.492s)\n",
      "Batch 300/485, loss: 0.3077823786251247 (0.491s)\n",
      "Batch 450/485, loss: 0.1947818470187485 (0.489s)\n",
      "Batch 485/485, loss: 0.24903521721384356 (0.115s)\n",
      "F1 score: 0.8992935235939729\n",
      "\n",
      "-----Epoch 127/250-----\n",
      "Batch 150/485, loss: 0.28483044925145806 (0.492s)\n",
      "Batch 300/485, loss: 0.21973890205224356 (0.490s)\n",
      "Batch 450/485, loss: 0.2590365084819496 (0.488s)\n",
      "Batch 485/485, loss: 0.26513766489390816 (0.114s)\n",
      "F1 score: 0.8984083255762972\n",
      "\n",
      "-----Epoch 128/250-----\n",
      "Batch 150/485, loss: 0.20023157039036354 (0.492s)\n",
      "Batch 300/485, loss: 0.3330766329728067 (0.490s)\n",
      "Batch 450/485, loss: 0.24447679664939642 (0.489s)\n",
      "Batch 485/485, loss: 0.2521959216999156 (0.114s)\n",
      "F1 score: 0.8778558956328136\n",
      "\n",
      "-----Epoch 129/250-----\n",
      "Batch 150/485, loss: 0.28106609772269925 (0.492s)\n",
      "Batch 300/485, loss: 0.19470886858490608 (0.490s)\n",
      "Batch 450/485, loss: 0.21901759105424087 (0.489s)\n",
      "Batch 485/485, loss: 0.5465265479471002 (0.114s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.8986864804827123\n",
      "\n",
      "-----Epoch 130/250-----\n",
      "Batch 150/485, loss: 0.3164939221056799 (0.492s)\n",
      "Batch 300/485, loss: 0.15268604772165417 (0.489s)\n",
      "Batch 450/485, loss: 0.2959321764484048 (0.489s)\n",
      "Batch 485/485, loss: 0.23813728777957813 (0.115s)\n",
      "F1 score: 0.9000751906534292\n",
      "\n",
      "-----Epoch 131/250-----\n",
      "Batch 150/485, loss: 0.2610199878457934 (0.491s)\n",
      "Batch 300/485, loss: 0.21191912331618368 (0.490s)\n",
      "Batch 450/485, loss: 0.2846264705869059 (0.489s)\n",
      "Batch 485/485, loss: 0.26424434315413237 (0.114s)\n",
      "F1 score: 0.9033785431543107\n",
      "\n",
      "-----Epoch 132/250-----\n",
      "Batch 150/485, loss: 0.2825977773157259 (0.493s)\n",
      "Batch 300/485, loss: 0.25601031004761654 (0.490s)\n",
      "Batch 450/485, loss: 0.2608653980990251 (0.489s)\n",
      "Batch 485/485, loss: 0.08890548686363868 (0.114s)\n",
      "F1 score: 0.9030453774496167\n",
      "\n",
      "-----Epoch 133/250-----\n",
      "Batch 150/485, loss: 0.25956623373242715 (0.492s)\n",
      "Batch 300/485, loss: 0.2969839307169119 (0.490s)\n",
      "Batch 450/485, loss: 0.22174161847059926 (0.489s)\n",
      "Batch 485/485, loss: 0.1897257073117154 (0.114s)\n",
      "F1 score: 0.9005852538469936\n",
      "\n",
      "-----Epoch 134/250-----\n",
      "Batch 150/485, loss: 0.21036832665093244 (0.492s)\n",
      "Batch 300/485, loss: 0.33159841146009666 (0.489s)\n",
      "Batch 450/485, loss: 0.2018568106430272 (0.489s)\n",
      "Batch 485/485, loss: 0.33520212692341633 (0.114s)\n",
      "F1 score: 0.9013057666201983\n",
      "\n",
      "-----Epoch 135/250-----\n",
      "Batch 150/485, loss: 0.19802812733997902 (0.492s)\n",
      "Batch 300/485, loss: 0.3538873023229341 (0.490s)\n",
      "Batch 450/485, loss: 0.23239642653924725 (0.489s)\n",
      "Batch 485/485, loss: 0.1560582942196301 (0.114s)\n",
      "F1 score: 0.902972140830203\n",
      "\n",
      "-----Epoch 136/250-----\n",
      "Batch 150/485, loss: 0.23779682651162148 (0.491s)\n",
      "Batch 300/485, loss: 0.31655124786620337 (0.489s)\n",
      "Batch 450/485, loss: 0.233457936088865 (0.489s)\n",
      "Batch 485/485, loss: 0.15300169165379235 (0.114s)\n",
      "F1 score: 0.8983201003820724\n",
      "\n",
      "-----Epoch 137/250-----\n",
      "Batch 150/485, loss: 0.27611663510091605 (0.491s)\n",
      "Batch 300/485, loss: 0.29793866293815274 (0.490s)\n",
      "Batch 450/485, loss: 0.20916846490154664 (0.489s)\n",
      "Batch 485/485, loss: 0.14601533583232335 (0.114s)\n",
      "F1 score: 0.9070989697088904\n",
      "\n",
      "-----Epoch 138/250-----\n",
      "Batch 150/485, loss: 0.3197061683796346 (0.492s)\n",
      "Batch 300/485, loss: 0.19927272482775152 (0.490s)\n",
      "Batch 450/485, loss: 0.2714778644343217 (0.489s)\n",
      "Batch 485/485, loss: 0.1138886423515422 (0.114s)\n",
      "F1 score: 0.9083566609143341\n",
      "\n",
      "-----Epoch 139/250-----\n",
      "Batch 150/485, loss: 0.21608755054573217 (0.492s)\n",
      "Batch 300/485, loss: 0.2975207460423311 (0.490s)\n",
      "Batch 450/485, loss: 0.25143420495092866 (0.488s)\n",
      "Batch 485/485, loss: 0.21654911738421237 (0.114s)\n",
      "F1 score: 0.9106971124860225\n",
      "\n",
      "-----Epoch 140/250-----\n",
      "Batch 150/485, loss: 0.21964376663478713 (0.491s)\n",
      "Batch 300/485, loss: 0.2649945663381368 (0.490s)\n",
      "Batch 450/485, loss: 0.26395500934993227 (0.489s)\n",
      "Batch 485/485, loss: 0.29185689043785845 (0.114s)\n",
      "F1 score: 0.9097876918603797\n",
      "\n",
      "-----Epoch 141/250-----\n",
      "Batch 150/485, loss: 0.13962992903776467 (0.493s)\n",
      "Batch 300/485, loss: 0.2534842249347518 (0.490s)\n",
      "Batch 450/485, loss: 0.3488103707196812 (0.489s)\n",
      "Batch 485/485, loss: 0.33717080924127785 (0.114s)\n",
      "F1 score: 0.9042855553871911\n",
      "\n",
      "-----Epoch 142/250-----\n",
      "Batch 150/485, loss: 0.27368921947975955 (0.491s)\n",
      "Batch 300/485, loss: 0.25429338701690235 (0.489s)\n",
      "Batch 450/485, loss: 0.22410791544864575 (0.489s)\n",
      "Batch 485/485, loss: 0.2930740269965359 (0.114s)\n",
      "F1 score: 0.9048437415183415\n",
      "\n",
      "-----Epoch 143/250-----\n",
      "Batch 150/485, loss: 0.36224207813851533 (0.492s)\n",
      "Batch 300/485, loss: 0.26060201661040383 (0.490s)\n",
      "Batch 450/485, loss: 0.1434134296234697 (0.489s)\n",
      "Batch 485/485, loss: 0.22285847948598012 (0.114s)\n",
      "F1 score: 0.9070224593335886\n",
      "\n",
      "-----Epoch 144/250-----\n",
      "Batch 150/485, loss: 0.2259237471750627 (0.493s)\n",
      "Batch 300/485, loss: 0.24795147333604595 (0.489s)\n",
      "Batch 450/485, loss: 0.2861513567250222 (0.489s)\n",
      "Batch 485/485, loss: 0.24719504322856664 (0.114s)\n",
      "F1 score: 0.9097104931726523\n",
      "\n",
      "-----Epoch 145/250-----\n",
      "Batch 150/485, loss: 0.15926578189556798 (0.491s)\n",
      "Batch 300/485, loss: 0.24576254307292403 (0.490s)\n",
      "Batch 450/485, loss: 0.3666238715872169 (0.490s)\n",
      "Batch 485/485, loss: 0.1882460808115346 (0.114s)\n",
      "F1 score: 0.9116623059998039\n",
      "\n",
      "-----Epoch 146/250-----\n",
      "Batch 150/485, loss: 0.2971395857321719 (0.492s)\n",
      "Batch 300/485, loss: 0.2475099503217886 (0.490s)\n",
      "Batch 450/485, loss: 0.23878927395679056 (0.489s)\n",
      "Batch 485/485, loss: 0.1373771261157734 (0.114s)\n",
      "F1 score: 0.9114298417665067\n",
      "\n",
      "-----Epoch 147/250-----\n",
      "Batch 150/485, loss: 0.3026636011774341 (0.492s)\n",
      "Batch 300/485, loss: 0.29649418730599186 (0.490s)\n",
      "Batch 450/485, loss: 0.19074491247224312 (0.489s)\n",
      "Batch 485/485, loss: 0.10240310836317283 (0.114s)\n",
      "F1 score: 0.9136711988836721\n",
      "\n",
      "-----Epoch 148/250-----\n",
      "Batch 150/485, loss: 0.24615760790805022 (0.492s)\n",
      "Batch 300/485, loss: 0.30028162885457277 (0.490s)\n",
      "Batch 450/485, loss: 0.25441646356135605 (0.489s)\n",
      "Batch 485/485, loss: 0.08784050523702587 (0.114s)\n",
      "F1 score: 0.9063911578030681\n",
      "\n",
      "-----Epoch 149/250-----\n",
      "Batch 150/485, loss: 0.297196989490961 (0.493s)\n",
      "Batch 300/485, loss: 0.21647647176558774 (0.490s)\n",
      "Batch 450/485, loss: 0.2747989472746849 (0.490s)\n",
      "Batch 485/485, loss: 0.13301688718742558 (0.115s)\n",
      "F1 score: 0.9124691772590223\n",
      "\n",
      "-----Epoch 150/250-----\n",
      "Batch 150/485, loss: 0.22598432620366415 (0.492s)\n",
      "Batch 300/485, loss: 0.25793976700554294 (0.490s)\n",
      "Batch 450/485, loss: 0.2828971523232758 (0.489s)\n",
      "Batch 485/485, loss: 0.22860360872000457 (0.114s)\n",
      "Saved to /home/hice1/khom9/CSE 8803 BMI Final Project/models/cnn_model-1.pth\n",
      "F1 score: 0.9069857388814345\n",
      "\n",
      "-----Epoch 151/250-----\n",
      "Batch 150/485, loss: 0.2695733592597147 (0.493s)\n",
      "Batch 300/485, loss: 0.24315565168857575 (0.492s)\n",
      "Batch 450/485, loss: 0.26978970898936194 (0.491s)\n",
      "Batch 485/485, loss: 0.12922541256994008 (0.114s)\n",
      "F1 score: 0.9150335430028886\n",
      "\n",
      "-----Epoch 152/250-----\n",
      "Batch 150/485, loss: 0.27379382083502907 (0.493s)\n",
      "Batch 300/485, loss: 0.2482360206358135 (0.491s)\n",
      "Batch 450/485, loss: 0.2307457425724715 (0.490s)\n",
      "Batch 485/485, loss: 0.2438516858166882 (0.115s)\n",
      "F1 score: 0.918650958751259\n",
      "\n",
      "-----Epoch 153/250-----\n",
      "Batch 150/485, loss: 0.2970476926397532 (0.492s)\n",
      "Batch 300/485, loss: 0.2710349808602283 (0.490s)\n",
      "Batch 450/485, loss: 0.22075050687417389 (0.490s)\n",
      "Batch 485/485, loss: 0.20928251551730293 (0.114s)\n",
      "F1 score: 0.8944379890732632\n",
      "\n",
      "-----Epoch 154/250-----\n",
      "Batch 150/485, loss: 0.2654560886416584 (0.492s)\n",
      "Batch 300/485, loss: 0.21016600426286458 (0.490s)\n",
      "Batch 450/485, loss: 0.3030706969772776 (0.489s)\n",
      "Batch 485/485, loss: 0.1649645453984184 (0.115s)\n",
      "F1 score: 0.9089379032668616\n",
      "\n",
      "-----Epoch 155/250-----\n",
      "Batch 150/485, loss: 0.24949015345734854 (0.493s)\n",
      "Batch 300/485, loss: 0.32483993521891535 (0.490s)\n",
      "Batch 450/485, loss: 0.1989114063543578 (0.489s)\n",
      "Batch 485/485, loss: 0.1645977031705635 (0.114s)\n",
      "F1 score: 0.9145650085772797\n",
      "\n",
      "-----Epoch 156/250-----\n",
      "Batch 150/485, loss: 0.2679427361053725 (0.493s)\n",
      "Batch 300/485, loss: 0.21406242064200343 (0.490s)\n",
      "Batch 450/485, loss: 0.22852844774723052 (0.490s)\n",
      "Batch 485/485, loss: 0.4222869926930538 (0.115s)\n",
      "F1 score: 0.9186472835457213\n",
      "\n",
      "-----Epoch 157/250-----\n",
      "Batch 150/485, loss: 0.26954286224208773 (0.493s)\n",
      "Batch 300/485, loss: 0.28096426094571747 (0.490s)\n",
      "Batch 450/485, loss: 0.18932047286381323 (0.490s)\n",
      "Batch 485/485, loss: 0.2950268111590828 (0.114s)\n",
      "F1 score: 0.9193940652863816\n",
      "\n",
      "-----Epoch 158/250-----\n",
      "Batch 150/485, loss: 0.2791674730057518 (0.491s)\n",
      "Batch 300/485, loss: 0.2675725014383594 (0.491s)\n",
      "Batch 450/485, loss: 0.24129393299110233 (0.490s)\n",
      "Batch 485/485, loss: 0.08318892355476107 (0.115s)\n",
      "F1 score: 0.9213434091663436\n",
      "\n",
      "-----Epoch 159/250-----\n",
      "Batch 150/485, loss: 0.1879242670815438 (0.492s)\n",
      "Batch 300/485, loss: 0.268374518432344 (0.490s)\n",
      "Batch 450/485, loss: 0.3132339538540691 (0.490s)\n",
      "Batch 485/485, loss: 0.1604012353611844 (0.115s)\n",
      "F1 score: 0.9220085591949334\n",
      "\n",
      "-----Epoch 160/250-----\n",
      "Batch 150/485, loss: 0.3701866423866401 (0.493s)\n",
      "Batch 300/485, loss: 0.1339940954030802 (0.490s)\n",
      "Batch 450/485, loss: 0.25179463223554194 (0.490s)\n",
      "Batch 485/485, loss: 0.22050774081477098 (0.115s)\n",
      "F1 score: 0.9219771586903738\n",
      "\n",
      "-----Epoch 161/250-----\n",
      "Batch 150/485, loss: 0.27091607795562594 (0.492s)\n",
      "Batch 300/485, loss: 0.2495792724036922 (0.490s)\n",
      "Batch 450/485, loss: 0.24928692842523256 (0.490s)\n",
      "Batch 485/485, loss: 0.1652427763012903 (0.114s)\n",
      "F1 score: 0.9222454284551345\n",
      "\n",
      "-----Epoch 162/250-----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 150/485, loss: 0.2378459068077306 (0.493s)\n",
      "Batch 300/485, loss: 0.19146127164984741 (0.490s)\n",
      "Batch 450/485, loss: 0.3230713261446605 (0.489s)\n",
      "Batch 485/485, loss: 0.23426851090043782 (0.114s)\n",
      "F1 score: 0.9227710951689116\n",
      "\n",
      "-----Epoch 163/250-----\n",
      "Batch 150/485, loss: 0.2829858940374106 (0.492s)\n",
      "Batch 300/485, loss: 0.2251232286884139 (0.490s)\n",
      "Batch 450/485, loss: 0.27144279749753575 (0.490s)\n",
      "Batch 485/485, loss: 0.11796788752877287 (0.114s)\n",
      "F1 score: 0.9232140166775802\n",
      "\n",
      "-----Epoch 164/250-----\n",
      "Batch 150/485, loss: 0.28981190879518787 (0.491s)\n",
      "Batch 300/485, loss: 0.2878675990489622 (0.492s)\n",
      "Batch 450/485, loss: 0.2023795798017333 (0.495s)\n",
      "Batch 485/485, loss: 0.11358126497694425 (0.115s)\n",
      "F1 score: 0.924096513263601\n",
      "\n",
      "-----Epoch 165/250-----\n",
      "Batch 150/485, loss: 0.2823651225414748 (0.497s)\n",
      "Batch 300/485, loss: 0.26680626354180276 (0.495s)\n",
      "Batch 450/485, loss: 0.22002405177491408 (0.494s)\n",
      "Batch 485/485, loss: 0.15930965856782028 (0.116s)\n",
      "F1 score: 0.9246278751224847\n",
      "\n",
      "-----Epoch 166/250-----\n",
      "Batch 150/485, loss: 0.2003472906863317 (0.498s)\n",
      "Batch 300/485, loss: 0.19826227005881567 (0.494s)\n",
      "Batch 450/485, loss: 0.33841872013173996 (0.495s)\n",
      "Batch 485/485, loss: 0.2952838377361851 (0.116s)\n",
      "F1 score: 0.9249779207020079\n",
      "\n",
      "-----Epoch 167/250-----\n",
      "Batch 150/485, loss: 0.16577473361976444 (0.497s)\n",
      "Batch 300/485, loss: 0.22853936931118368 (0.495s)\n",
      "Batch 450/485, loss: 0.29819637903322777 (0.495s)\n",
      "Batch 485/485, loss: 0.4981116166870509 (0.116s)\n",
      "F1 score: 0.9242617332529832\n",
      "\n",
      "-----Epoch 168/250-----\n",
      "Batch 150/485, loss: 0.2703820021543652 (0.497s)\n",
      "Batch 300/485, loss: 0.2836622307108094 (0.492s)\n",
      "Batch 450/485, loss: 0.22667369768954815 (0.489s)\n",
      "Batch 485/485, loss: 0.10786222553412829 (0.114s)\n",
      "F1 score: 0.9253856591174052\n",
      "\n",
      "-----Epoch 169/250-----\n",
      "Batch 150/485, loss: 0.31786409022907414 (0.492s)\n",
      "Batch 300/485, loss: 0.14679115052334965 (0.490s)\n",
      "Batch 450/485, loss: 0.30742050919681785 (0.490s)\n",
      "Batch 485/485, loss: 0.14365507255175283 (0.114s)\n",
      "F1 score: 0.9259397878580229\n",
      "\n",
      "-----Epoch 170/250-----\n",
      "Batch 150/485, loss: 0.1661602941000213 (0.492s)\n",
      "Batch 300/485, loss: 0.2350084226516386 (0.491s)\n",
      "Batch 450/485, loss: 0.32774927460898956 (0.490s)\n",
      "Batch 485/485, loss: 0.3326843736028033 (0.115s)\n",
      "F1 score: 0.9249239968173771\n",
      "\n",
      "-----Epoch 171/250-----\n",
      "Batch 150/485, loss: 0.23557728598515193 (0.492s)\n",
      "Batch 300/485, loss: 0.2778308462537825 (0.489s)\n",
      "Batch 450/485, loss: 0.22975365763530134 (0.490s)\n",
      "Batch 485/485, loss: 0.3425741645374468 (0.114s)\n",
      "F1 score: 0.9233672707402963\n",
      "\n",
      "-----Epoch 172/250-----\n",
      "Batch 150/485, loss: 0.1939903013346096 (0.492s)\n",
      "Batch 300/485, loss: 0.30328502346140646 (0.490s)\n",
      "Batch 450/485, loss: 0.23894597188569605 (0.490s)\n",
      "Batch 485/485, loss: 0.30418644079140256 (0.115s)\n",
      "F1 score: 0.9241886177922102\n",
      "\n",
      "-----Epoch 173/250-----\n",
      "Batch 150/485, loss: 0.2930673950786392 (0.492s)\n",
      "Batch 300/485, loss: 0.17414402366926274 (0.490s)\n",
      "Batch 450/485, loss: 0.271897145792221 (0.489s)\n",
      "Batch 485/485, loss: 0.302104533649981 (0.114s)\n",
      "F1 score: 0.9226832131413742\n",
      "\n",
      "-----Epoch 174/250-----\n",
      "Batch 150/485, loss: 0.1832180201075971 (0.492s)\n",
      "Batch 300/485, loss: 0.33153276721946895 (0.490s)\n",
      "Batch 450/485, loss: 0.23503323183084526 (0.490s)\n",
      "Batch 485/485, loss: 0.265524031820574 (0.114s)\n",
      "F1 score: 0.9178420067295161\n",
      "\n",
      "-----Epoch 175/250-----\n",
      "Batch 150/485, loss: 0.28332690662393967 (0.493s)\n",
      "Batch 300/485, loss: 0.3004933146573603 (0.490s)\n",
      "Batch 450/485, loss: 0.16748102327343076 (0.490s)\n",
      "Batch 485/485, loss: 0.23648506007822498 (0.115s)\n",
      "Saved to /home/hice1/khom9/CSE 8803 BMI Final Project/models/cnn_model-1.pth\n",
      "F1 score: 0.9246875215819501\n",
      "\n",
      "-----Epoch 176/250-----\n",
      "Batch 150/485, loss: 0.33328581089153886 (0.497s)\n",
      "Batch 300/485, loss: 0.18247264514366784 (0.495s)\n",
      "Batch 450/485, loss: 0.21872994115886588 (0.495s)\n",
      "Batch 485/485, loss: 0.29825926198224934 (0.115s)\n",
      "F1 score: 0.926230704478453\n",
      "\n",
      "-----Epoch 177/250-----\n",
      "Batch 150/485, loss: 0.22172809108470878 (0.497s)\n",
      "Batch 300/485, loss: 0.18704625056435664 (0.496s)\n",
      "Batch 450/485, loss: 0.3301501965026061 (0.496s)\n",
      "Batch 485/485, loss: 0.3300511222066624 (0.115s)\n",
      "F1 score: 0.9278137480295545\n",
      "\n",
      "-----Epoch 178/250-----\n",
      "Batch 150/485, loss: 0.2873499782538662 (0.494s)\n",
      "Batch 300/485, loss: 0.33809946339266994 (0.493s)\n",
      "Batch 450/485, loss: 0.12715688835208613 (0.493s)\n",
      "Batch 485/485, loss: 0.22631551598065666 (0.115s)\n",
      "F1 score: 0.9275318231501025\n",
      "\n",
      "-----Epoch 179/250-----\n",
      "Batch 150/485, loss: 0.2106742435041815 (0.494s)\n",
      "Batch 300/485, loss: 0.36520261586022873 (0.493s)\n",
      "Batch 450/485, loss: 0.1914909068867564 (0.492s)\n",
      "Batch 485/485, loss: 0.15434679931827955 (0.115s)\n",
      "F1 score: 0.9275813173384295\n",
      "\n",
      "-----Epoch 180/250-----\n",
      "Batch 150/485, loss: 0.2658543105299274 (0.493s)\n",
      "Batch 300/485, loss: 0.1960918953983734 (0.492s)\n",
      "Batch 450/485, loss: 0.2561945925652981 (0.492s)\n",
      "Batch 485/485, loss: 0.37506072010312763 (0.115s)\n",
      "F1 score: 0.9253271456091401\n",
      "\n",
      "-----Epoch 181/250-----\n",
      "Batch 150/485, loss: 0.2512424614528815 (0.495s)\n",
      "Batch 300/485, loss: 0.33796291588805616 (0.492s)\n",
      "Batch 450/485, loss: 0.17455420981471736 (0.491s)\n",
      "Batch 485/485, loss: 0.1695962758202638 (0.115s)\n",
      "F1 score: 0.9277020128659244\n",
      "\n",
      "-----Epoch 182/250-----\n",
      "Batch 150/485, loss: 0.23925473259761929 (0.494s)\n",
      "Batch 300/485, loss: 0.17840246330636242 (0.492s)\n",
      "Batch 450/485, loss: 0.2193320015259087 (0.491s)\n",
      "Batch 485/485, loss: 0.713171613628843 (0.115s)\n",
      "F1 score: 0.9283368242042938\n",
      "\n",
      "-----Epoch 183/250-----\n",
      "Batch 150/485, loss: 0.2730811820210268 (0.494s)\n",
      "Batch 300/485, loss: 0.25063490548481543 (0.495s)\n",
      "Batch 450/485, loss: 0.2539894083297501 (0.494s)\n",
      "Batch 485/485, loss: 0.1511719969234296 (0.116s)\n",
      "F1 score: 0.9190895350364365\n",
      "\n",
      "-----Epoch 184/250-----\n",
      "Batch 150/485, loss: 0.20168099100701511 (0.497s)\n",
      "Batch 300/485, loss: 0.25484497894222535 (0.494s)\n",
      "Batch 450/485, loss: 0.3184370050486177 (0.493s)\n",
      "Batch 485/485, loss: 0.11890912797035916 (0.115s)\n",
      "F1 score: 0.9285298819864742\n",
      "\n",
      "-----Epoch 185/250-----\n",
      "Batch 150/485, loss: 0.2156553898161898 (0.495s)\n",
      "Batch 300/485, loss: 0.3388738994750505 (0.493s)\n",
      "Batch 450/485, loss: 0.16945478561023872 (0.492s)\n",
      "Batch 485/485, loss: 0.32640474139313613 (0.115s)\n",
      "F1 score: 0.9300731526589984\n",
      "\n",
      "-----Epoch 186/250-----\n",
      "Batch 150/485, loss: 0.24179590536902348 (0.495s)\n",
      "Batch 300/485, loss: 0.22230897108403344 (0.493s)\n",
      "Batch 450/485, loss: 0.2923407599174728 (0.493s)\n",
      "Batch 485/485, loss: 0.18560822512954472 (0.115s)\n",
      "F1 score: 0.9295588210601103\n",
      "\n",
      "-----Epoch 187/250-----\n",
      "Batch 150/485, loss: 0.2877402897893141 (0.495s)\n",
      "Batch 300/485, loss: 0.3033742296292136 (0.493s)\n",
      "Batch 450/485, loss: 0.1564717090688646 (0.492s)\n",
      "Batch 485/485, loss: 0.2203388374431857 (0.115s)\n",
      "F1 score: 0.9298115885160064\n",
      "\n",
      "-----Epoch 188/250-----\n",
      "Batch 150/485, loss: 0.2150389191477249 (0.495s)\n",
      "Batch 300/485, loss: 0.1728842049961289 (0.493s)\n",
      "Batch 450/485, loss: 0.3481509217681984 (0.492s)\n",
      "Batch 485/485, loss: 0.26618533014718976 (0.115s)\n",
      "F1 score: 0.9307823674507623\n",
      "\n",
      "-----Epoch 189/250-----\n",
      "Batch 150/485, loss: 0.2045651859883219 (0.496s)\n",
      "Batch 300/485, loss: 0.29896581136311096 (0.493s)\n",
      "Batch 450/485, loss: 0.25057252248749134 (0.492s)\n",
      "Batch 485/485, loss: 0.19862012188615544 (0.115s)\n",
      "F1 score: 0.9279984415249415\n",
      "\n",
      "-----Epoch 190/250-----\n",
      "Batch 150/485, loss: 0.22962143005492786 (0.494s)\n",
      "Batch 300/485, loss: 0.27776198108680544 (0.493s)\n",
      "Batch 450/485, loss: 0.2308290818395714 (0.492s)\n",
      "Batch 485/485, loss: 0.2576359241402575 (0.115s)\n",
      "F1 score: 0.9305332929657009\n",
      "\n",
      "-----Epoch 191/250-----\n",
      "Batch 150/485, loss: 0.20250607348047198 (0.495s)\n",
      "Batch 300/485, loss: 0.21687511382314065 (0.493s)\n",
      "Batch 450/485, loss: 0.28418269590785106 (0.492s)\n",
      "Batch 485/485, loss: 0.40664749217352697 (0.115s)\n",
      "F1 score: 0.9304518774664731\n",
      "\n",
      "-----Epoch 192/250-----\n",
      "Batch 150/485, loss: 0.19593624758534134 (0.496s)\n",
      "Batch 300/485, loss: 0.22775700584830094 (0.493s)\n",
      "Batch 450/485, loss: 0.2905980014087011 (0.493s)\n",
      "Batch 485/485, loss: 0.3515553738389696 (0.115s)\n",
      "F1 score: 0.9319973696330048\n",
      "\n",
      "-----Epoch 193/250-----\n",
      "Batch 150/485, loss: 0.23089280300463239 (0.494s)\n",
      "Batch 300/485, loss: 0.24198518017306925 (0.493s)\n",
      "Batch 450/485, loss: 0.24054439029190688 (0.493s)\n",
      "Batch 485/485, loss: 0.35805362250123707 (0.115s)\n",
      "F1 score: 0.9304802328550172\n",
      "\n",
      "-----Epoch 194/250-----\n",
      "Batch 150/485, loss: 0.20265757396817208 (0.495s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300/485, loss: 0.2810885322000831 (0.493s)\n",
      "Batch 450/485, loss: 0.296603719368577 (0.492s)\n",
      "Batch 485/485, loss: 0.06449656845735652 (0.115s)\n",
      "F1 score: 0.9317879505539085\n",
      "\n",
      "-----Epoch 195/250-----\n",
      "Batch 150/485, loss: 0.2432819292259713 (0.494s)\n",
      "Batch 300/485, loss: 0.22111562225812426 (0.493s)\n",
      "Batch 450/485, loss: 0.2589030019038667 (0.492s)\n",
      "Batch 485/485, loss: 0.3096841770756458 (0.115s)\n",
      "F1 score: 0.9317001973957636\n",
      "\n",
      "-----Epoch 196/250-----\n",
      "Batch 150/485, loss: 0.313543106984968 (0.495s)\n",
      "Batch 300/485, loss: 0.2375033964837591 (0.493s)\n",
      "Batch 450/485, loss: 0.1783804824234297 (0.492s)\n",
      "Batch 485/485, loss: 0.2783825267638479 (0.115s)\n",
      "F1 score: 0.932469230766485\n",
      "\n",
      "-----Epoch 197/250-----\n",
      "Batch 150/485, loss: 0.21846636440294484 (0.495s)\n",
      "Batch 300/485, loss: 0.2845190263275678 (0.493s)\n",
      "Batch 450/485, loss: 0.24095750481200714 (0.493s)\n",
      "Batch 485/485, loss: 0.22076048755220004 (0.115s)\n",
      "F1 score: 0.9311444221791572\n",
      "\n",
      "-----Epoch 198/250-----\n",
      "Batch 150/485, loss: 0.16891736728139223 (0.495s)\n",
      "Batch 300/485, loss: 0.32844476968981323 (0.493s)\n",
      "Batch 450/485, loss: 0.2459345824768146 (0.492s)\n",
      "Batch 485/485, loss: 0.24482077889676604 (0.115s)\n",
      "F1 score: 0.9310194540790545\n",
      "\n",
      "-----Epoch 199/250-----\n",
      "Batch 150/485, loss: 0.32715604969300327 (0.494s)\n",
      "Batch 300/485, loss: 0.18807064095822473 (0.493s)\n",
      "Batch 450/485, loss: 0.21573605420067907 (0.492s)\n",
      "Batch 485/485, loss: 0.2892706628622753 (0.115s)\n",
      "F1 score: 0.931271577751762\n",
      "\n",
      "-----Epoch 200/250-----\n",
      "Batch 150/485, loss: 0.21247422572846214 (0.495s)\n",
      "Batch 300/485, loss: 0.24870152913344404 (0.493s)\n",
      "Batch 450/485, loss: 0.2797690747274707 (0.492s)\n",
      "Batch 485/485, loss: 0.25425728928031666 (0.115s)\n",
      "Saved to /home/hice1/khom9/CSE 8803 BMI Final Project/models/cnn_model-1.pth\n",
      "F1 score: 0.9266792593831599\n",
      "\n",
      "-----Epoch 201/250-----\n",
      "Batch 150/485, loss: 0.27995481028221547 (0.496s)\n",
      "Batch 300/485, loss: 0.26513967039994896 (0.494s)\n",
      "Batch 450/485, loss: 0.1856779662674914 (0.493s)\n",
      "Batch 485/485, loss: 0.266585581975856 (0.115s)\n",
      "F1 score: 0.9336937858232225\n",
      "\n",
      "-----Epoch 202/250-----\n",
      "Batch 150/485, loss: 0.23055570609246692 (0.495s)\n",
      "Batch 300/485, loss: 0.1871179916181912 (0.493s)\n",
      "Batch 450/485, loss: 0.330379399281616 (0.492s)\n",
      "Batch 485/485, loss: 0.18089198037715895 (0.115s)\n",
      "F1 score: 0.9348970908800199\n",
      "\n",
      "-----Epoch 203/250-----\n",
      "Batch 150/485, loss: 0.2068194945404927 (0.494s)\n",
      "Batch 300/485, loss: 0.31295824330920974 (0.492s)\n",
      "Batch 450/485, loss: 0.22649844556115567 (0.492s)\n",
      "Batch 485/485, loss: 0.1868722130810576 (0.115s)\n",
      "F1 score: 0.935132904412058\n",
      "\n",
      "-----Epoch 204/250-----\n",
      "Batch 150/485, loss: 0.1985650750497977 (0.495s)\n",
      "Batch 300/485, loss: 0.30757512708194557 (0.493s)\n",
      "Batch 450/485, loss: 0.23134951128313938 (0.493s)\n",
      "Batch 485/485, loss: 0.2227801353936749 (0.115s)\n",
      "F1 score: 0.9359810895318222\n",
      "\n",
      "-----Epoch 205/250-----\n",
      "Batch 150/485, loss: 0.19826608003427584 (0.495s)\n",
      "Batch 300/485, loss: 0.2706314163391168 (0.493s)\n",
      "Batch 450/485, loss: 0.2517228650239607 (0.493s)\n",
      "Batch 485/485, loss: 0.29354335102917894 (0.115s)\n",
      "F1 score: 0.9358054181816535\n",
      "\n",
      "-----Epoch 206/250-----\n",
      "Batch 150/485, loss: 0.36116312237146 (0.494s)\n",
      "Batch 300/485, loss: 0.19684038316830993 (0.493s)\n",
      "Batch 450/485, loss: 0.1994113632120813 (0.492s)\n",
      "Batch 485/485, loss: 0.136656882760248 (0.115s)\n",
      "F1 score: 0.9352660996496911\n",
      "\n",
      "-----Epoch 207/250-----\n",
      "Batch 150/485, loss: 0.2766343145103504 (0.495s)\n",
      "Batch 300/485, loss: 0.25133925524229805 (0.493s)\n",
      "Batch 450/485, loss: 0.2109176041558385 (0.493s)\n",
      "Batch 485/485, loss: 0.21478344232642224 (0.115s)\n",
      "F1 score: 0.9360724268624188\n",
      "\n",
      "-----Epoch 208/250-----\n",
      "Batch 150/485, loss: 0.27601651022831597 (0.496s)\n",
      "Batch 300/485, loss: 0.2818557313627874 (0.492s)\n",
      "Batch 450/485, loss: 0.1874320034387832 (0.492s)\n",
      "Batch 485/485, loss: 0.19211357778736524 (0.115s)\n",
      "F1 score: 0.9352359767718421\n",
      "\n",
      "-----Epoch 209/250-----\n",
      "Batch 150/485, loss: 0.3083005120512098 (0.495s)\n",
      "Batch 300/485, loss: 0.1903929389330248 (0.493s)\n",
      "Batch 450/485, loss: 0.20226856652492037 (0.492s)\n",
      "Batch 485/485, loss: 0.37558198430176293 (0.115s)\n",
      "F1 score: 0.9366631630695567\n",
      "\n",
      "-----Epoch 210/250-----\n",
      "Batch 150/485, loss: 0.23288173315736155 (0.494s)\n",
      "Batch 300/485, loss: 0.32551959783459705 (0.493s)\n",
      "Batch 450/485, loss: 0.171539720032985 (0.492s)\n",
      "Batch 485/485, loss: 0.2506848266880427 (0.115s)\n",
      "F1 score: 0.9367872565442319\n",
      "\n",
      "-----Epoch 211/250-----\n",
      "Batch 150/485, loss: 0.27032255223797014 (0.495s)\n",
      "Batch 300/485, loss: 0.20366753537828725 (0.493s)\n",
      "Batch 450/485, loss: 0.28529660230036824 (0.492s)\n",
      "Batch 485/485, loss: 0.12681419208113637 (0.115s)\n",
      "F1 score: 0.9366701523224439\n",
      "\n",
      "-----Epoch 212/250-----\n",
      "Batch 150/485, loss: 0.22649330344516783 (0.494s)\n",
      "Batch 300/485, loss: 0.21920477501116692 (0.493s)\n",
      "Batch 450/485, loss: 0.29342782650142907 (0.492s)\n",
      "Batch 485/485, loss: 0.20838270895183086 (0.115s)\n",
      "F1 score: 0.9368522390071133\n",
      "\n",
      "-----Epoch 213/250-----\n",
      "Batch 150/485, loss: 0.2880689972732216 (0.495s)\n",
      "Batch 300/485, loss: 0.22499173611092071 (0.493s)\n",
      "Batch 450/485, loss: 0.251361314477399 (0.493s)\n",
      "Batch 485/485, loss: 0.10297735329451306 (0.115s)\n",
      "F1 score: 0.9369064784493324\n",
      "\n",
      "-----Epoch 214/250-----\n",
      "Batch 150/485, loss: 0.3764504968763019 (0.496s)\n",
      "Batch 300/485, loss: 0.2095077003336822 (0.493s)\n",
      "Batch 450/485, loss: 0.17098115470881264 (0.492s)\n",
      "Batch 485/485, loss: 0.1321073071498956 (0.115s)\n",
      "F1 score: 0.9378645061708578\n",
      "\n",
      "-----Epoch 215/250-----\n",
      "Batch 150/485, loss: 0.2351476656847323 (0.494s)\n",
      "Batch 300/485, loss: 0.2881432456150651 (0.495s)\n",
      "Batch 450/485, loss: 0.21312816040124744 (0.492s)\n",
      "Batch 485/485, loss: 0.2180397855250963 (0.115s)\n",
      "F1 score: 0.9371770553290357\n",
      "\n",
      "-----Epoch 216/250-----\n",
      "Batch 150/485, loss: 0.26028708156663927 (0.494s)\n",
      "Batch 300/485, loss: 0.15370307967377206 (0.492s)\n",
      "Batch 450/485, loss: 0.35261085784528406 (0.491s)\n",
      "Batch 485/485, loss: 0.0883148383083088 (0.115s)\n",
      "F1 score: 0.9373489207522283\n",
      "\n",
      "-----Epoch 217/250-----\n",
      "Batch 150/485, loss: 0.29381204589114834 (0.493s)\n",
      "Batch 300/485, loss: 0.1752757304503272 (0.492s)\n",
      "Batch 450/485, loss: 0.20898432351493587 (0.491s)\n",
      "Batch 485/485, loss: 0.4677172110549041 (0.115s)\n",
      "F1 score: 0.9374280262051521\n",
      "\n",
      "-----Epoch 218/250-----\n",
      "Batch 150/485, loss: 0.18372151166666298 (0.494s)\n",
      "Batch 300/485, loss: 0.2976241758745164 (0.492s)\n",
      "Batch 450/485, loss: 0.28608121786266566 (0.491s)\n",
      "Batch 485/485, loss: 0.09263809384512049 (0.115s)\n",
      "F1 score: 0.936054896164389\n",
      "\n",
      "-----Epoch 219/250-----\n",
      "Batch 150/485, loss: 0.18022843093145638 (0.494s)\n",
      "Batch 300/485, loss: 0.27292673438166576 (0.492s)\n",
      "Batch 450/485, loss: 0.2682762678169335 (0.492s)\n",
      "Batch 485/485, loss: 0.28065570761848774 (0.115s)\n",
      "F1 score: 0.9374032876316748\n",
      "\n",
      "-----Epoch 220/250-----\n",
      "Batch 150/485, loss: 0.20888663173187524 (0.494s)\n",
      "Batch 300/485, loss: 0.18902673292594652 (0.492s)\n",
      "Batch 450/485, loss: 0.3252445236096779 (0.492s)\n",
      "Batch 485/485, loss: 0.2697983581439725 (0.115s)\n",
      "F1 score: 0.9381776439556617\n",
      "\n",
      "-----Epoch 221/250-----\n",
      "Batch 150/485, loss: 0.1938942508865148 (0.494s)\n",
      "Batch 300/485, loss: 0.22838513479568065 (0.492s)\n",
      "Batch 450/485, loss: 0.26192559958746037 (0.491s)\n",
      "Batch 485/485, loss: 0.43712312197312714 (0.115s)\n",
      "F1 score: 0.9377145739123326\n",
      "\n",
      "-----Epoch 222/250-----\n",
      "Batch 150/485, loss: 0.21449567549706747 (0.494s)\n",
      "Batch 300/485, loss: 0.23424766144249587 (0.492s)\n",
      "Batch 450/485, loss: 0.2699132510212561 (0.492s)\n",
      "Batch 485/485, loss: 0.2874572541963841 (0.115s)\n",
      "F1 score: 0.9379351376402225\n",
      "\n",
      "-----Epoch 223/250-----\n",
      "Batch 150/485, loss: 0.19877583120173464 (0.495s)\n",
      "Batch 300/485, loss: 0.22521423421489695 (0.492s)\n",
      "Batch 450/485, loss: 0.3094994665340831 (0.491s)\n",
      "Batch 485/485, loss: 0.22497192277972186 (0.115s)\n",
      "F1 score: 0.9381427730476792\n",
      "\n",
      "-----Epoch 224/250-----\n",
      "Batch 150/485, loss: 0.2547318726964295 (0.493s)\n",
      "Batch 300/485, loss: 0.20199170841525 (0.493s)\n",
      "Batch 450/485, loss: 0.307578726131469 (0.493s)\n",
      "Batch 485/485, loss: 0.09551782167649694 (0.115s)\n",
      "F1 score: 0.9378249531067392\n",
      "\n",
      "-----Epoch 225/250-----\n",
      "Batch 150/485, loss: 0.25096951580761623 (0.496s)\n",
      "Batch 300/485, loss: 0.18757334272842854 (0.492s)\n",
      "Batch 450/485, loss: 0.28037370888826746 (0.492s)\n",
      "Batch 485/485, loss: 0.28282996095450863 (0.115s)\n",
      "Saved to /home/hice1/khom9/CSE 8803 BMI Final Project/models/cnn_model-1.pth\n",
      "F1 score: 0.9388329922618814\n",
      "\n",
      "-----Epoch 226/250-----\n",
      "Batch 150/485, loss: 0.2872911536584919 (0.494s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300/485, loss: 0.22822639731379846 (0.493s)\n",
      "Batch 450/485, loss: 0.20538683472666888 (0.492s)\n",
      "Batch 485/485, loss: 0.2746062432016645 (0.115s)\n",
      "F1 score: 0.9390294714337001\n",
      "\n",
      "-----Epoch 227/250-----\n",
      "Batch 150/485, loss: 0.22855848529686532 (0.494s)\n",
      "Batch 300/485, loss: 0.16557145308858404 (0.492s)\n",
      "Batch 450/485, loss: 0.34507560472625 (0.491s)\n",
      "Batch 485/485, loss: 0.20794247355578202 (0.115s)\n",
      "F1 score: 0.9383035976952715\n",
      "\n",
      "-----Epoch 228/250-----\n",
      "Batch 150/485, loss: 0.3001110146365439 (0.494s)\n",
      "Batch 300/485, loss: 0.266548830276976 (0.491s)\n",
      "Batch 450/485, loss: 0.16315671956942726 (0.491s)\n",
      "Batch 485/485, loss: 0.2422189111023077 (0.115s)\n",
      "F1 score: 0.9377362958450286\n",
      "\n",
      "-----Epoch 229/250-----\n",
      "Batch 150/485, loss: 0.2596049742043639 (0.494s)\n",
      "Batch 300/485, loss: 0.20007752488212038 (0.492s)\n",
      "Batch 450/485, loss: 0.21412854157698652 (0.491s)\n",
      "Batch 485/485, loss: 0.4725816511549056 (0.115s)\n",
      "F1 score: 0.9395430314529355\n",
      "\n",
      "-----Epoch 230/250-----\n",
      "Batch 150/485, loss: 0.3787793209264055 (0.495s)\n",
      "Batch 300/485, loss: 0.19569256652301797 (0.492s)\n",
      "Batch 450/485, loss: 0.18215760050807148 (0.490s)\n",
      "Batch 485/485, loss: 0.11935878369425024 (0.115s)\n",
      "F1 score: 0.9389642163064124\n",
      "\n",
      "-----Epoch 231/250-----\n",
      "Batch 150/485, loss: 0.17167967499854664 (0.494s)\n",
      "Batch 300/485, loss: 0.2078748094042142 (0.493s)\n",
      "Batch 450/485, loss: 0.3620555973906691 (0.491s)\n",
      "Batch 485/485, loss: 0.18422828864838395 (0.115s)\n",
      "F1 score: 0.9388583486874205\n",
      "\n",
      "-----Epoch 232/250-----\n",
      "Batch 150/485, loss: 0.2094112972387423 (0.493s)\n",
      "Batch 300/485, loss: 0.27515285566759606 (0.492s)\n",
      "Batch 450/485, loss: 0.23125122129761924 (0.491s)\n",
      "Batch 485/485, loss: 0.2905454142817429 (0.115s)\n",
      "F1 score: 0.9394922401335842\n",
      "\n",
      "-----Epoch 233/250-----\n",
      "Batch 150/485, loss: 0.17959594537659238 (0.494s)\n",
      "Batch 300/485, loss: 0.25587895630082735 (0.491s)\n",
      "Batch 450/485, loss: 0.29236013581976295 (0.490s)\n",
      "Batch 485/485, loss: 0.23757941326392548 (0.115s)\n",
      "F1 score: 0.9392570538708335\n",
      "\n",
      "-----Epoch 234/250-----\n",
      "Batch 150/485, loss: 0.18388363249599934 (0.494s)\n",
      "Batch 300/485, loss: 0.2724253781201939 (0.491s)\n",
      "Batch 450/485, loss: 0.265036732936278 (0.491s)\n",
      "Batch 485/485, loss: 0.28819695315989 (0.115s)\n",
      "F1 score: 0.9389853812834219\n",
      "\n",
      "-----Epoch 235/250-----\n",
      "Batch 150/485, loss: 0.2701634035150831 (0.494s)\n",
      "Batch 300/485, loss: 0.1563052767732491 (0.492s)\n",
      "Batch 450/485, loss: 0.29990277539473026 (0.491s)\n",
      "Batch 485/485, loss: 0.24512638229477618 (0.115s)\n",
      "F1 score: 0.9396622293772826\n",
      "\n",
      "-----Epoch 236/250-----\n",
      "Batch 150/485, loss: 0.2386878485077371 (0.494s)\n",
      "Batch 300/485, loss: 0.3218535563861951 (0.492s)\n",
      "Batch 450/485, loss: 0.20833348345011474 (0.490s)\n",
      "Batch 485/485, loss: 0.0629383420704731 (0.114s)\n",
      "F1 score: 0.9394633868211528\n",
      "\n",
      "-----Epoch 237/250-----\n",
      "Batch 150/485, loss: 0.22202756704917798 (0.494s)\n",
      "Batch 300/485, loss: 0.3293984821625054 (0.492s)\n",
      "Batch 450/485, loss: 0.18148345000265786 (0.491s)\n",
      "Batch 485/485, loss: 0.21557344111746976 (0.115s)\n",
      "F1 score: 0.9397891525379984\n",
      "\n",
      "-----Epoch 238/250-----\n",
      "Batch 150/485, loss: 0.22738799372067053 (0.494s)\n",
      "Batch 300/485, loss: 0.2088417050195858 (0.492s)\n",
      "Batch 450/485, loss: 0.29227067029103637 (0.491s)\n",
      "Batch 485/485, loss: 0.2320608947825219 (0.115s)\n",
      "F1 score: 0.9404220806094069\n",
      "\n",
      "-----Epoch 239/250-----\n",
      "Batch 150/485, loss: 0.2049613334176441 (0.493s)\n",
      "Batch 300/485, loss: 0.34310561905304593 (0.492s)\n",
      "Batch 450/485, loss: 0.18082998608549436 (0.491s)\n",
      "Batch 485/485, loss: 0.25247722872133765 (0.115s)\n",
      "F1 score: 0.9403131037332515\n",
      "\n",
      "-----Epoch 240/250-----\n",
      "Batch 150/485, loss: 0.308560220344613 (0.493s)\n",
      "Batch 300/485, loss: 0.22183925300836563 (0.491s)\n",
      "Batch 450/485, loss: 0.22804070821031927 (0.491s)\n",
      "Batch 485/485, loss: 0.10487376524667655 (0.115s)\n",
      "F1 score: 0.9398023678326816\n",
      "\n",
      "-----Epoch 241/250-----\n",
      "Batch 150/485, loss: 0.29530369233650466 (0.494s)\n",
      "Batch 300/485, loss: 0.18112745703353236 (0.492s)\n",
      "Batch 450/485, loss: 0.2482065199393158 (0.492s)\n",
      "Batch 485/485, loss: 0.24744155855317201 (0.115s)\n",
      "F1 score: 0.9404686229712198\n",
      "\n",
      "-----Epoch 242/250-----\n",
      "Batch 150/485, loss: 0.2935535868552203 (0.495s)\n",
      "Batch 300/485, loss: 0.18077948476808767 (0.493s)\n",
      "Batch 450/485, loss: 0.26211373241773495 (0.492s)\n",
      "Batch 485/485, loss: 0.19817430938460998 (0.115s)\n",
      "F1 score: 0.940498490394313\n",
      "\n",
      "-----Epoch 243/250-----\n",
      "Batch 150/485, loss: 0.2182654423483958 (0.494s)\n",
      "Batch 300/485, loss: 0.21522260060689102 (0.493s)\n",
      "Batch 450/485, loss: 0.29526619453138364 (0.492s)\n",
      "Batch 485/485, loss: 0.23196152734703251 (0.115s)\n",
      "F1 score: 0.9401329001501312\n",
      "\n",
      "-----Epoch 244/250-----\n",
      "Batch 150/485, loss: 0.260691652726382 (0.493s)\n",
      "Batch 300/485, loss: 0.23161515636369587 (0.493s)\n",
      "Batch 450/485, loss: 0.21277178932602206 (0.492s)\n",
      "Batch 485/485, loss: 0.33063717376041624 (0.115s)\n",
      "F1 score: 0.9401378761749758\n",
      "\n",
      "-----Epoch 245/250-----\n",
      "Batch 150/485, loss: 0.26099133301836747 (0.494s)\n",
      "Batch 300/485, loss: 0.2732793186077227 (0.492s)\n",
      "Batch 450/485, loss: 0.20690765713496753 (0.491s)\n",
      "Batch 485/485, loss: 0.17481425172383233 (0.115s)\n",
      "F1 score: 0.9404071333820117\n",
      "\n",
      "-----Epoch 246/250-----\n",
      "Batch 150/485, loss: 0.22264418891553456 (0.494s)\n",
      "Batch 300/485, loss: 0.2992138612953325 (0.492s)\n",
      "Batch 450/485, loss: 0.2103296854564299 (0.491s)\n",
      "Batch 485/485, loss: 0.21089149230558957 (0.115s)\n",
      "F1 score: 0.9405104496461016\n",
      "\n",
      "-----Epoch 247/250-----\n",
      "Batch 150/485, loss: 0.2736702312498043 (0.497s)\n",
      "Batch 300/485, loss: 0.21993679749779405 (0.494s)\n",
      "Batch 450/485, loss: 0.2000726110006993 (0.494s)\n",
      "Batch 485/485, loss: 0.3743877088917153 (0.115s)\n",
      "F1 score: 0.9410841110286446\n",
      "\n",
      "-----Epoch 248/250-----\n",
      "Batch 150/485, loss: 0.2807297793915495 (0.495s)\n",
      "Batch 300/485, loss: 0.24553112397591273 (0.493s)\n",
      "Batch 450/485, loss: 0.19840472062118353 (0.492s)\n",
      "Batch 485/485, loss: 0.24354700800031423 (0.115s)\n",
      "F1 score: 0.9408377569454635\n",
      "\n",
      "-----Epoch 249/250-----\n",
      "Batch 150/485, loss: 0.181922972338895 (0.493s)\n",
      "Batch 300/485, loss: 0.24959553676036497 (0.492s)\n",
      "Batch 450/485, loss: 0.2992125469849755 (0.491s)\n",
      "Batch 485/485, loss: 0.228220518771559 (0.115s)\n",
      "F1 score: 0.9383934102462179\n",
      "\n",
      "-----Epoch 250/250-----\n",
      "Batch 150/485, loss: 0.18699793330859393 (0.494s)\n",
      "Batch 300/485, loss: 0.29299044409145913 (0.493s)\n",
      "Batch 450/485, loss: 0.2523895434383303 (0.492s)\n",
      "Batch 485/485, loss: 0.2136953234073839 (0.115s)\n",
      "Saved to /home/hice1/khom9/CSE 8803 BMI Final Project/models/cnn_model-1.pth\n",
      "F1 score: 0.9404050575643492\n"
     ]
    }
   ],
   "source": [
    "epochs = 250\n",
    "train_model(model, optimizer, d, loss_fn, epochs=epochs, batch_size=batch_size, save_freq=25, \n",
    "            save_path=save_path, scheduler=scheduler, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e6ef550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /home/hice1/khom9/CSE 8803 BMI Final Project/models/cnn_model-1.pth\n"
     ]
    }
   ],
   "source": [
    "save_model(save_path, model, optimizer, epochs)\n",
    "print(f'Saved to {save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1a0c335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.323808181856293e-06\n",
      "tensor([[ -28.9006,    0.0000],\n",
      "        [-314.0933,    0.0000],\n",
      "        [-252.4385,    0.0000],\n",
      "        [-258.5004,    0.0000],\n",
      "        [-152.8970,    0.0000],\n",
      "        [-298.6354,    0.0000],\n",
      "        [-238.3664,    0.0000],\n",
      "        [ -37.4053,    0.0000],\n",
      "        [ -57.9946,    0.0000],\n",
      "        [-275.8915,    0.0000],\n",
      "        [-116.5328,    0.0000],\n",
      "        [-206.7182,    0.0000],\n",
      "        [-305.9601,    0.0000],\n",
      "        [-253.1639,    0.0000],\n",
      "        [-209.7269,    0.0000],\n",
      "        [-311.6870,    0.0000],\n",
      "        [-189.1265,    0.0000],\n",
      "        [-265.6090,    0.0000],\n",
      "        [-291.2330,    0.0000],\n",
      "        [-302.5996,    0.0000],\n",
      "        [ -21.7756,    0.0000],\n",
      "        [-312.1428,    0.0000],\n",
      "        [-232.0197,    0.0000],\n",
      "        [-298.3479,    0.0000],\n",
      "        [-274.4721,    0.0000],\n",
      "        [-312.5026,    0.0000],\n",
      "        [-216.7874,    0.0000],\n",
      "        [-212.0948,    0.0000],\n",
      "        [-303.3674,    0.0000],\n",
      "        [-283.8804,    0.0000],\n",
      "        [ -17.4958,    0.0000],\n",
      "        [-129.3488,    0.0000],\n",
      "        [  -9.1027,    0.0000],\n",
      "        [ -44.5149,    0.0000],\n",
      "        [  -8.3774,    0.0000],\n",
      "        [ -73.3590,    0.0000],\n",
      "        [-138.3448,    0.0000],\n",
      "        [-278.9894,    0.0000],\n",
      "        [-295.7919,    0.0000],\n",
      "        [-239.8816,    0.0000],\n",
      "        [-236.5785,    0.0000],\n",
      "        [-267.6623,    0.0000],\n",
      "        [-284.6236,    0.0000],\n",
      "        [-217.9879,    0.0000],\n",
      "        [-222.3197,    0.0000],\n",
      "        [-198.2960,    0.0000],\n",
      "        [-310.9459,    0.0000],\n",
      "        [-309.9630,    0.0000],\n",
      "        [-202.2700,    0.0000],\n",
      "        [-284.1284,    0.0000],\n",
      "        [-283.7503,    0.0000],\n",
      "        [-261.9402,    0.0000],\n",
      "        [ -48.9964,    0.0000],\n",
      "        [ -64.8675,    0.0000],\n",
      "        [-153.6277,    0.0000],\n",
      "        [-184.6678,    0.0000],\n",
      "        [-220.8214,    0.0000],\n",
      "        [ -64.4095,    0.0000],\n",
      "        [-316.5202,    0.0000],\n",
      "        [-176.8928,    0.0000],\n",
      "        [-162.5083,    0.0000],\n",
      "        [ -77.0630,    0.0000],\n",
      "        [-249.3615,    0.0000],\n",
      "        [-215.0324,    0.0000],\n",
      "        [-320.6690,    0.0000],\n",
      "        [ -58.0239,    0.0000],\n",
      "        [-220.7194,    0.0000],\n",
      "        [ -69.1437,    0.0000],\n",
      "        [-242.4854,    0.0000],\n",
      "        [-196.1304,    0.0000],\n",
      "        [-303.9160,    0.0000],\n",
      "        [-239.3961,    0.0000],\n",
      "        [-233.6369,    0.0000],\n",
      "        [-310.2300,    0.0000],\n",
      "        [  -8.9162,    0.0000],\n",
      "        [ -29.4615,    0.0000],\n",
      "        [  19.3732,    1.0000],\n",
      "        [-186.2396,    0.0000],\n",
      "        [ -11.7247,    0.0000],\n",
      "        [ -22.6839,    0.0000],\n",
      "        [ -37.4777,    0.0000],\n",
      "        [ -41.5627,    0.0000],\n",
      "        [ -74.0011,    0.0000],\n",
      "        [-141.2470,    0.0000],\n",
      "        [-152.9720,    0.0000],\n",
      "        [-123.4238,    0.0000],\n",
      "        [ -99.8829,    0.0000],\n",
      "        [ -65.3223,    0.0000],\n",
      "        [-132.4225,    0.0000],\n",
      "        [ -14.2830,    0.0000],\n",
      "        [-207.3401,    0.0000]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "i = 8721 #18622\n",
    "txt, label = d[i]\n",
    "label = label.unsqueeze(0)\n",
    "print(loss_fn(model(txt.to(DEVICE)).detach(), label.to(DEVICE)).item())\n",
    "print(torch.cat([model(txt.to(DEVICE)).detach(), label.to(DEVICE)]).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e36ea3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = (torch.sigmoid(batch_predict(model, d.abst_data, device=DEVICE).detach().cpu()) > 0.5).type(torch.float)\n",
    "true = d.labels.to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "831c7a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total avg loss: 1.1084296340429656\n"
     ]
    }
   ],
   "source": [
    "total_loss = []\n",
    "loss_fn_cpu = loss_fn.cpu()\n",
    "for i in range(len(d)):\n",
    "    total_loss.append(loss_fn_cpu(pred[i].unsqueeze(0), true[i].unsqueeze(0)).item())\n",
    "    \n",
    "print(f'Total avg loss: {np.mean(total_loss)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f669161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2267     193.282471\n",
       "22573    186.985901\n",
       "18414    186.191101\n",
       "17995    185.235703\n",
       "18534    181.800720\n",
       "18548    178.970963\n",
       "21263    178.187866\n",
       "17761    177.895233\n",
       "18622    177.773224\n",
       "11919     98.311630\n",
       "20998     97.643471\n",
       "11239     97.067375\n",
       "18514     90.423332\n",
       "18483     90.423332\n",
       "9062      90.150620\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.Series(total_loss)\n",
    "x.sort_values(ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d865e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88817891 0.         0.75       1.         1.         1.\n",
      " 0.88888889 0.94444444 0.91957105 1.         0.95833333 1.\n",
      " 1.         0.9        0.8        0.         0.92105263 0.75\n",
      " 1.         0.94444444 0.91646778 0.         0.5        0.\n",
      " 0.875      0.         1.         1.         0.         0.5\n",
      " 0.96189367 0.88679245 0.82608696 0.84210526 0.91082803 0.8961039\n",
      " 0.91509434 1.         0.         1.         0.3        1.\n",
      " 0.09090909 0.         0.         0.8        0.         0.\n",
      " 1.         0.         1.         0.         1.         0.86956522\n",
      " 0.87179487 1.         0.80357143 0.88095238 0.75       0.77272727\n",
      " 1.         0.93650794 1.         0.92857143 1.         0.84210526\n",
      " 1.         0.98113208 0.85       0.77777778 0.         0.88888889\n",
      " 0.75       0.         0.89034425 0.86062718 0.86538462 0.94736842\n",
      " 0.93817557 0.93912753 0.88297173 0.8590898  0.82847896 0.8960396\n",
      " 0.86842105 0.79761905 0.92380952 0.90721649 0.82608696 0.91335265\n",
      " 0.90816327]\n",
      "Total precision: 0.9225320781507335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(precision_score(true, pred, average=None))\n",
    "print(f'Total precision: {precision_score(true, pred, average=\"weighted\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f164fc17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97887324 0.         0.75       1.         1.         0.5\n",
      " 0.8        0.95280236 0.95543175 0.92307692 0.96842105 0.52380952\n",
      " 0.66666667 0.6        0.8        0.         0.67307692 0.14285714\n",
      " 1.         0.425      0.93203883 0.         0.16666667 0.\n",
      " 0.29166667 0.         0.36363636 0.76923077 0.         1.\n",
      " 0.95372051 0.95918367 0.99580713 0.96969697 0.91961415 0.94520548\n",
      " 0.98979592 1.         0.         0.6        0.5        0.5\n",
      " 0.2        0.         0.         0.44444444 0.         0.\n",
      " 1.         0.         0.5        0.         0.84615385 0.90909091\n",
      " 0.87179487 0.66666667 0.91836735 0.96103896 0.5        0.85\n",
      " 0.91176471 0.9516129  0.75       0.76470588 0.57142857 0.9039548\n",
      " 0.66666667 0.94545455 0.77272727 0.73684211 0.         0.8\n",
      " 0.54545455 0.         0.9906577  0.97244094 0.91836735 0.9\n",
      " 0.97022566 0.98701754 0.99925595 0.99915086 0.99610895 1.\n",
      " 1.         1.         0.97487437 0.94964029 0.98701299 0.99569337\n",
      " 0.93684211]\n",
      "Total recall: 0.9716742135604716\n"
     ]
    }
   ],
   "source": [
    "print(recall_score(true, pred, average=None))\n",
    "print(f'Total recall: {recall_score(true, pred, average=\"weighted\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5a9f732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.945287335378083\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(true, pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "851d848b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total accuracy: 0.8397849462365592\n"
     ]
    }
   ],
   "source": [
    "print(f'Total accuracy: {accuracy_score(true, pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9aec5fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved word embeddings to /home/hice1/khom9/CSE 8803 BMI Final Project/wordvectors/abstracts200_trained_normalized_1.wordvectors\n"
     ]
    }
   ],
   "source": [
    "wv_tuned = gensim.models.KeyedVectors.load(EMBED_KEYS_PATH, mmap='r')\n",
    "wv_tuned.vectors = model.embedding.weight.data.detach().cpu().numpy()\n",
    "\n",
    "wv_tuned.vectors = np.clip(wv_tuned.vectors, a_min=0, a_max=1.)\n",
    "\n",
    "wv_tuned.save(wv_out_path)\n",
    "print(f'Saved word embeddings to {wv_out_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d6160c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote max layer activations to /home/hice1/khom9/CSE 8803 BMI Final Project/models/cnn_model-1-max-activations.pkl\n"
     ]
    }
   ],
   "source": [
    "activations = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for txt, label in (d):\n",
    "        outputs = list(model(txt.to(DEVICE), all_outputs_max=True))\n",
    "        activations.append(outputs)\n",
    "activations = torch.tensor(activations)\n",
    "\n",
    "max_act = torch.max(activations, axis=0)[0]\n",
    "max_act_dict = OrderedDict(zip(list(dict(model.named_modules()).keys())[1:], max_act))\n",
    "\n",
    "output = open(act_path, 'wb')\n",
    "pickle.dump(max_act_dict, output)\n",
    "output.close()\n",
    "print(f'Wrote max layer activations to {act_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226a1b54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
